"""
SFCç­ç»„åˆæ ¼ç‡æ•°æ®æ¸…æ´—ETLè„šæœ¬
å¤„ç†æ¯æ—¥ç­ç»„åˆæ ¼ç‡æ•°æ®ï¼Œç”Ÿæˆè´¨é‡åˆ†ææŠ¥å‘Š
"""

import os
import sys
import pandas as pd
import numpy as np
import glob
import json
from datetime import datetime, timedelta
import logging
import argparse
from pathlib import Path
import re

# æ·»åŠ ETLå·¥å…·å‡½æ•°
sys.path.append(os.path.dirname(__file__))
from etl_utils import load_config, setup_logging, save_to_parquet, read_sharepoint_excel

def extract_report_date_from_filename(filename: str) -> datetime:
    """
    ä»æ–‡ä»¶åæå–æŠ¥è¡¨æ—¥æœŸ
    IGPR-20251202080001 -> 2025-12-02 08:00:01
    """
    try:
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… IGPR-YYYYMMDDHHMMSS æ ¼å¼
        pattern = r'IGPR-(\d{4})(\d{2})(\d{2})(\d{2})(\d{2})(\d{2})'
        match = re.search(pattern, filename)
        
        if match:
            year, month, day, hour, minute, second = match.groups()
            return datetime(int(year), int(month), int(day), int(hour), int(minute), int(second))
        else:
            # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›æ–‡ä»¶ä¿®æ”¹æ—¶é—´ä½œä¸ºå¤‡é€‰
            logging.warning(f"æ— æ³•ä»æ–‡ä»¶åæå–æ—¥æœŸ: {filename}ï¼Œä½¿ç”¨æ–‡ä»¶ä¿®æ”¹æ—¶é—´")
            return datetime.now()  # æˆ–è€…è¿”å› None
            
    except Exception as e:
        logging.warning(f"è§£ææ–‡ä»¶åæ—¥æœŸå¤±è´¥ {filename}: {e}")
        return datetime.now()  # æˆ–è€…è¿”å› None

def read_team_passrate_data(config: dict) -> pd.DataFrame:
    """
    è¯»å–ç­ç»„åˆæ ¼ç‡æ•°æ®
    æ”¯æŒå¤šæ–‡ä»¶è¯»å–å’Œåˆå¹¶
    """
    source_cfg = config.get("source", {})
    data_path = source_cfg.get("team_passrate_path", "")
    
    if not data_path:
        raise ValueError("æœªé…ç½®ç­ç»„åˆæ ¼ç‡æ•°æ®è·¯å¾„")
    
    logging.info(f"è¯»å–ç­ç»„åˆæ ¼ç‡æ•°æ®: {data_path}")
    
    # å¤„ç†é€šé…ç¬¦è·¯å¾„
    if "*" in data_path or "?" in data_path:
        data_files = glob.glob(data_path)
        if not data_files:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°åŒ¹é…çš„æ•°æ®æ–‡ä»¶: {data_path}")
        
        # æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæœ€æ–°æ–‡ä»¶ä¼˜å…ˆï¼‰
        data_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
        logging.info(f"æ‰¾åˆ° {len(data_files)} ä¸ªç­ç»„åˆæ ¼ç‡æ•°æ®æ–‡ä»¶ï¼ŒæŒ‰ä¿®æ”¹æ—¶é—´æ’åº")
        
        # æµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶æ–‡ä»¶æ•°é‡
        test_cfg = config.get("test", {})
        test_enabled = test_cfg.get("enabled", False)
        max_files = test_cfg.get("max_files", 20) if test_enabled else len(data_files)
        
        if test_enabled and len(data_files) > max_files:
            data_files = data_files[:max_files]
            logging.info(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šä»…è¯»å–å‰ {max_files} ä¸ªæœ€æ–°æ–‡ä»¶")
        
        # è¯»å–æ‰€æœ‰æ–‡ä»¶å¹¶åˆå¹¶
        dfs = []
        for i, file_path in enumerate(data_files, start=1):
            try:
                mod_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                logging.info(f"è¯»å–æ–‡ä»¶ {i}/{len(data_files)}: {os.path.basename(file_path)} (ä¿®æ”¹æ—¶é—´: {mod_time})")
                df = read_sharepoint_excel(file_path)
                if not df.empty:
                    df['source_file'] = os.path.basename(file_path)
                    df['file_mod_time'] = datetime.fromtimestamp(os.path.getmtime(file_path))
                    
                    # ä»æ–‡ä»¶åæå–æŠ¥è¡¨æ—¥æœŸ
                    filename = os.path.basename(file_path)
                    report_date = extract_report_date_from_filename(filename)
                    df['æŠ¥è¡¨æ—¥æœŸ'] = report_date
                    
                    dfs.append(df)
            except Exception as e:
                logging.warning(f"è¯»å–æ–‡ä»¶å¤±è´¥ {i}/{len(data_files)}: {file_path}: {e}")
        
        if not dfs:
            raise ValueError("æ‰€æœ‰ç­ç»„åˆæ ¼ç‡æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥")
        
        combined_df = pd.concat(dfs, ignore_index=True)
        logging.info(f"åˆå¹¶åç­ç»„åˆæ ¼ç‡æ•°æ®è¡Œæ•°: {len(combined_df)}")
        return combined_df
    
    else:
        # å•æ–‡ä»¶å¤„ç†
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"ç­ç»„åˆæ ¼ç‡æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        
        df = read_sharepoint_excel(data_path)
        if not df.empty:
            # ä»æ–‡ä»¶åæå–æŠ¥è¡¨æ—¥æœŸ
            filename = os.path.basename(data_path)
            report_date = extract_report_date_from_filename(filename)
            df['æŠ¥è¡¨æ—¥æœŸ'] = report_date
            
        return df

def validate_team_passrate_data(df: pd.DataFrame, config: dict) -> bool:
    """
    éªŒè¯ç­ç»„åˆæ ¼ç‡æ•°æ®å®Œæ•´æ€§
    """
    logging.info("å¼€å§‹éªŒè¯ç­ç»„åˆæ ¼ç‡æ•°æ®å®Œæ•´æ€§...")
    
    # æ£€æŸ¥å¿…è¦å­—æ®µ
    required_fields = config.get("processing", {}).get("required_fields", 
        ["æ‰¹æ¬¡å·", "äº§å“åºå·", "æ£€éªŒç»“æœ", "ç­ç»„"])
    
    missing_fields = [field for field in required_fields if field not in df.columns]
    if missing_fields:
        logging.error(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}")
        return False
    
    # æ£€æŸ¥æ•°æ®è¡Œæ•°
    if df.empty:
        logging.error("ç­ç»„åˆæ ¼ç‡æ•°æ®ä¸ºç©º")
        return False
    
    # æ£€æŸ¥å…³é”®å­—æ®µç¼ºå¤±ç‡
    for field in required_fields:
        missing_count = df[field].isnull().sum()
        missing_rate = missing_count / len(df) * 100
        if missing_rate > 5:  # ç¼ºå¤±ç‡è¶…è¿‡5%æŠ¥è­¦
            logging.warning(f"å­—æ®µ {field} ç¼ºå¤±ç‡è¾ƒé«˜: {missing_rate:.1f}% ({missing_count}/{len(df)})")
    
    logging.info(f"ç­ç»„åˆæ ¼ç‡æ•°æ®éªŒè¯é€šè¿‡: {len(df)} è¡Œè®°å½•")
    return True

def clean_team_passrate_data(df: pd.DataFrame, config: dict) -> pd.DataFrame:
    """
    æ¸…æ´—ç­ç»„åˆæ ¼ç‡æ•°æ®
    """
    logging.info("å¼€å§‹æ¸…æ´—ç­ç»„åˆæ ¼ç‡æ•°æ®...")
    
    result = df.copy()
    original_count = len(result)
    
    # 1. æ ‡å‡†åŒ–å­—æ®µåï¼ˆå»é™¤ç©ºæ ¼ï¼Œç»Ÿä¸€å‘½åï¼‰
    result.columns = result.columns.str.strip()
    
    # 2. æ ‡å‡†åŒ–æ‰¹æ¬¡å·ï¼šç§»é™¤IPQCKå‰ç¼€
    if 'æ‰¹æ¬¡å·' in result.columns:
        result['æ‰¹æ¬¡å·'] = result['æ‰¹æ¬¡å·'].astype(str).str.replace(r'^IPQCK', '', regex=True)
        logging.info("å·²æ ‡å‡†åŒ–æ‰¹æ¬¡å·ï¼šç§»é™¤IPQCKå‰ç¼€")
    
    # 2. å¤„ç†æœºåºŠè®¾å¤‡ç¼ºå¤±å€¼
    if 'æœºåºŠè®¾å¤‡' in result.columns:
        missing_equipment = result['æœºåºŠè®¾å¤‡'].isnull()
        if missing_equipment.any():
            # ä½¿ç”¨å·¥åºç¼–å·å¡«å……ç¼ºå¤±çš„æœºåºŠè®¾å¤‡ï¼Œæ˜¾å¼è½¬æ¢ä¸ºå­—ç¬¦ä¸²é¿å…dtypeè­¦å‘Š
            result['æœºåºŠè®¾å¤‡'] = result['æœºåºŠè®¾å¤‡'].astype('object')
            result.loc[missing_equipment, 'æœºåºŠè®¾å¤‡'] = result.loc[missing_equipment, 'å·¥åºç¼–å·'].astype(str)
            logging.info(f"å·²ç”¨å·¥åºç¼–å·å¡«å…… {missing_equipment.sum()} ä¸ªç¼ºå¤±çš„æœºåºŠè®¾å¤‡")
    
    # 3. å¤„ç†äº§å“åºå·ç¼ºå¤±å€¼
    if 'äº§å“åºå·' in result.columns:
        missing_serial = result['äº§å“åºå·'].isnull()
        if missing_serial.any():
            # ä½¿ç”¨è¡Œå·å¡«å……ç¼ºå¤±çš„äº§å“åºå·
            result.loc[missing_serial, 'äº§å“åºå·'] = result.loc[missing_serial].groupby('æ‰¹æ¬¡å·').cumcount() + 1
            logging.info(f"å·²å¡«å…… {missing_serial.sum()} ä¸ªç¼ºå¤±çš„äº§å“åºå·")
    
    # 4. æ ‡å‡†åŒ–æ£€éªŒç»“æœ
    if 'æ£€éªŒç»“æœ' in result.columns:
        # ç»Ÿä¸€æ£€éªŒç»“æœæ ¼å¼
        result['æ£€éªŒç»“æœ'] = result['æ£€éªŒç»“æœ'].str.strip().str.upper()
        valid_results = ['åˆæ ¼', 'ä¸åˆæ ¼', 'PASS', 'FAIL']
        invalid_results = ~result['æ£€éªŒç»“æœ'].isin(valid_results)
        if invalid_results.any():
            logging.warning(f"å‘ç° {invalid_results.sum()} ä¸ªå¼‚å¸¸æ£€éªŒç»“æœ")
    
    # 5. æ•°æ®ç±»å‹è½¬æ¢
    type_conversions = {
        'æœºåºŠè®¾å¤‡': 'str',
        'äº§å“åºå·': 'int64',
        'åˆæ ¼æ•°': 'int64',
        'ä¸åˆæ ¼æ•°': 'int64'
    }
    
    for col, dtype in type_conversions.items():
        if col in result.columns:
            try:
                result[col] = result[col].astype(dtype)
            except Exception as e:
                logging.warning(f"å­—æ®µ {col} ç±»å‹è½¬æ¢å¤±è´¥: {e}")
    
    # 6. ä¸šåŠ¡é”®å»é‡ï¼ˆä¿ç•™æœ€æ–°æ•°æ®ï¼‰
    before_dedup = len(result)
    
    # æ£€æŸ¥ä¸šåŠ¡é”®æ˜¯å¦å­˜åœ¨
    business_keys = ['æ‰¹æ¬¡å·', 'äº§å“åºå·', 'å·¥åºç¼–å·', 'å·¥åºåç§°']
    missing_keys = [key for key in business_keys if key not in result.columns]
    
    if missing_keys:
        logging.warning(f"ç¼ºå°‘ä¸šåŠ¡é”®å­—æ®µ {missing_keys}ï¼Œä½¿ç”¨æ‰¹æ¬¡+å·¥åº+ç­ç»„å»é‡")
        # å¦‚æœç¼ºå°‘äº§å“åºå·ï¼Œä½¿ç”¨æ‰¹æ¬¡+å·¥åº+ç­ç»„ä½œä¸ºå¤‡é€‰ä¸šåŠ¡é”®
        backup_keys = ['æ‰¹æ¬¡å·', 'å·¥åºç¼–å·', 'å·¥åºåç§°', 'ç­ç»„']
        backup_missing = [key for key in backup_keys if key not in result.columns]
        if backup_missing:
            logging.warning(f"ç¼ºå°‘å¤‡é€‰ä¸šåŠ¡é”®å­—æ®µ {backup_missing}ï¼Œä½¿ç”¨å…¨è¡Œå»é‡")
            result = result.drop_duplicates()
        else:
            result = result.drop_duplicates(subset=backup_keys, keep='first')
            logging.info(f"å¤‡é€‰ä¸šåŠ¡é”®å»é‡: åŸºäºæ‰¹æ¬¡+å·¥åº+ç­ç»„ç§»é™¤é‡å¤è®°å½•")
    else:
        # ä½¿ç”¨å®Œæ•´çš„ä¸šåŠ¡é”®å»é‡ï¼Œä¿ç•™æœ€æ–°æ–‡ä»¶çš„è®°å½•ï¼ˆkeep='first'å› ä¸ºå·²æŒ‰æ–‡ä»¶æ—¶é—´æ’åºï¼‰
        before_business_dedup = len(result)
        result = result.drop_duplicates(subset=business_keys, keep='first')
        
        business_dedup_count = before_business_dedup - len(result)
        logging.info(f"ä¸šåŠ¡é”®å»é‡: åŸºäºæ‰¹æ¬¡å·+äº§å“åºå·+å·¥åºç¼–å·+å·¥åºåç§°ç§»é™¤ {business_dedup_count} æ¡é‡å¤è®°å½•")
        
        # å¦‚æœä»æœ‰å®Œå…¨é‡å¤çš„è¡Œï¼Œå†è¿›è¡Œä¸€æ¬¡å…¨è¡Œå»é‡
        final_before = len(result)
        result = result.drop_duplicates()
        final_dedup_count = final_before - len(result)
        
        if final_dedup_count > 0:
            logging.info(f"å…¨è¡Œå»é‡: ç§»é™¤ {final_dedup_count} æ¡å®Œå…¨é‡å¤è®°å½•")
    
    removed_duplicates = before_dedup - len(result)
    logging.info(f"æ€»å»é‡ç»Ÿè®¡: ç§»é™¤ {removed_duplicates} æ¡é‡å¤è®°å½•")
    
    logging.info(f"ç­ç»„åˆæ ¼ç‡æ•°æ®æ¸…æ´—å®Œæˆ: {original_count} â†’ {len(result)} è¡Œ")
    return result

def aggregate_team_passrate_data_batch_level(df: pd.DataFrame, config: dict) -> pd.DataFrame:
    """
    æŒ‰æ‰¹æ¬¡è®¡ç®—çœŸå®åˆæ ¼ç‡ï¼šä»¥æ‰¹æ¬¡ä¸ºå•ä½ï¼Œä½¿ç”¨æœ€ç»ˆå·¥åºåˆæ ¼æ•°/ï¼ˆæœ€ç»ˆå·¥åºåˆæ ¼æ•°+æ‰€æœ‰å·¥åºä¸åˆæ ¼æ•°æ€»å’Œï¼‰
    è¿™æ ·èƒ½åæ˜ äº§å“åœ¨æ•´ä¸ªç”Ÿäº§è¿‡ç¨‹ä¸­çš„çœŸå®è´¨é‡
    """
    if df.empty:
        return df
    
    result = df.copy()
    logging.info("å¼€å§‹æŒ‰æ‰¹æ¬¡è®¡ç®—çœŸå®åˆæ ¼ç‡...")
    
    # æ£€æŸ¥å¿…è¦çš„å­—æ®µ
    required_fields = ['æ‰¹æ¬¡å·', 'äº§å“å·', 'å·¥åºç¼–å·', 'å·¥åºåç§°', 'ç­ç»„']
    missing_fields = [field for field in required_fields if field not in result.columns]
    
    if missing_fields:
        logging.error(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}")
        return pd.DataFrame()
    
    # å¤„ç†ä¸¤ç§æ£€éªŒç»“æœæ ¼å¼
    if 'æ£€éªŒç»“æœ' in result.columns and ('åˆæ ¼æ•°' not in result.columns or 'ä¸åˆæ ¼æ•°' not in result.columns):
        # æ ¼å¼1: ä¸ªäººäº§å“çº§æ£€éªŒç»“æœï¼Œéœ€è¦èšåˆè®¡ç®—
        logging.info("æ£€æµ‹åˆ°ä¸ªäººäº§å“çº§æ£€éªŒç»“æœï¼Œå¼€å§‹èšåˆè®¡ç®—...")
        
        # æ ‡å‡†åŒ–æ£€éªŒç»“æœ
        result['æ£€éªŒç»“æœ'] = result['æ£€éªŒç»“æœ'].str.strip().str.upper()
        result['æ˜¯å¦åˆæ ¼'] = result['æ£€éªŒç»“æœ'].isin(['åˆæ ¼', 'PASS'])
        
        # æŒ‰æ‰¹æ¬¡+äº§å“å·+å·¥åºç¼–å·+ç­ç»„èšåˆ
        operation_agg = result.groupby(['æ‰¹æ¬¡å·', 'äº§å“å·', 'å·¥åºç¼–å·', 'å·¥åºåç§°', 'ç­ç»„']).agg(
            åˆæ ¼æ•°=('æ˜¯å¦åˆæ ¼', 'sum'),
            ä¸åˆæ ¼æ•°=('æ˜¯å¦åˆæ ¼', lambda x: len(x) - x.sum()),
            äº§å“åºå·æ•°é‡=('äº§å“åºå·', 'count'),
            file_mod_time=('file_mod_time', 'max'),
            source_file=('source_file', lambda x: ', '.join(set(x.dropna().astype(str).str.strip())))
        ).reset_index()
        
    elif 'åˆæ ¼æ•°' in result.columns and 'ä¸åˆæ ¼æ•°' in result.columns:
        # æ ¼å¼2: å·²æœ‰æ‰¹æ¬¡çº§ç»Ÿè®¡æ•°æ®ï¼Œç›´æ¥èšåˆ
        logging.info("æ£€æµ‹åˆ°æ‰¹æ¬¡çº§ç»Ÿè®¡æ•°æ®ï¼Œç›´æ¥èšåˆ...")
        
        # ç¡®ä¿æ•°å€¼ç±»å‹
        result['åˆæ ¼æ•°'] = pd.to_numeric(result['åˆæ ¼æ•°'], errors='coerce').fillna(0)
        result['ä¸åˆæ ¼æ•°'] = pd.to_numeric(result['ä¸åˆæ ¼æ•°'], errors='coerce').fillna(0)
        
        # æŒ‰æ‰¹æ¬¡+äº§å“å·+å·¥åºç¼–å·+ç­ç»„èšåˆï¼Œä¿ç•™äº§å“åºå·ä¿¡æ¯
        operation_agg = result.groupby(['æ‰¹æ¬¡å·', 'äº§å“å·', 'å·¥åºç¼–å·', 'å·¥åºåç§°', 'ç­ç»„']).agg(
            åˆæ ¼æ•°=('åˆæ ¼æ•°', 'sum'),
            ä¸åˆæ ¼æ•°=('ä¸åˆæ ¼æ•°', 'sum'),
            äº§å“åºå·æ•°é‡=('äº§å“åºå·', 'count'),
            äº§å“åºå·=('äº§å“åºå·', 'first'),  # ä¿ç•™ç¬¬ä¸€ä¸ªäº§å“åºå·
            file_mod_time=('file_mod_time', 'max'),
            source_file=('source_file', lambda x: ', '.join(set(x.dropna().astype(str).str.strip())))
        ).reset_index()
        
    else:
        logging.error("æ— æ³•è¯†åˆ«æ£€éªŒç»“æœæ ¼å¼")
        return pd.DataFrame()
    
    logging.info(f"å·¥åºçº§èšåˆå®Œæˆ: {len(result)} â†’ {len(operation_agg)} æ¡è®°å½•")
    
    # è½¬æ¢å·¥åºç¼–å·ä¸ºæ•°å­—ä»¥ä¾¿æ¯”è¾ƒæœ€å¤§å€¼ï¼Œå¤„ç†æ— æ•ˆå€¼
    operation_agg['å·¥åºç¼–å·_æ•°å­—'] = pd.to_numeric(operation_agg['å·¥åºç¼–å·'], errors='coerce')
    
    # è¿‡æ»¤æ‰å·¥åºç¼–å·ä¸ºNaNçš„è®°å½•ï¼ˆè¿™äº›å¯èƒ½æ˜¯N/Aç­‰éæ•°å­—å€¼ï¼‰
    valid_operations = operation_agg.dropna(subset=['å·¥åºç¼–å·_æ•°å­—']).copy()
    
    if len(valid_operations) == 0:
        logging.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°å­—å·¥åºç¼–å·ï¼Œæ— æ³•è®¡ç®—æ‰¹æ¬¡çº§åˆæ ¼ç‡")
        return pd.DataFrame()
    
    if len(valid_operations) < len(operation_agg):
        logging.warning(f"è¿‡æ»¤æ‰ {len(operation_agg) - len(valid_operations)} æ¡æ— æ•ˆå·¥åºç¼–å·è®°å½•")
    
    # æŒ‰æ‰¹æ¬¡åˆ†ç»„ï¼Œè®¡ç®—æ‰¹æ¬¡çº§åˆæ ¼ç‡
    batch_results = []
    skipped_batches = 0
    
    for (batch_no, product_no), batch_group in valid_operations.groupby(['æ‰¹æ¬¡å·', 'äº§å“å·']):
        try:
            # æ‰¾åˆ°æœ€ç»ˆå·¥åºï¼ˆå·¥åºç¼–å·æœ€å¤§çš„å·¥åºï¼‰
            max_idx = batch_group['å·¥åºç¼–å·_æ•°å­—'].idxmax()
            final_operations = batch_group.loc[max_idx]
            
            # è®¡ç®—æ‰¹æ¬¡æ€»ä¸åˆæ ¼æ•°ï¼ˆæ‰€æœ‰å·¥åºçš„ä¸åˆæ ¼æ•°æ€»å’Œï¼‰
            total_unqualified = batch_group['ä¸åˆæ ¼æ•°'].sum()
            
            # æœ€ç»ˆå·¥åºçš„åˆæ ¼æ•°ï¼ˆå•ä¸ªç­ç»„çš„åˆæ ¼æ•°ï¼‰
            final_qualified = final_operations['åˆæ ¼æ•°']
            
            # è®¡ç®—æ‰¹æ¬¡çœŸå®åˆæ ¼ç‡ = æœ€ç»ˆå·¥åºåˆæ ¼æ•° / (æœ€ç»ˆå·¥åºåˆæ ¼æ•° + æ‰€æœ‰å·¥åºä¸åˆæ ¼æ•°æ€»å’Œ)
            total_defects = final_qualified + total_unqualified
            batch_pass_rate = (final_qualified / total_defects * 100).round(2) if total_defects > 0 else 0
            
            # è½¬æ¢ä¸ºç™¾åˆ†æ¯”æ ¼å¼ï¼ˆç¼©å°100å€ï¼‰
            batch_pass_rate_percentage = round(batch_pass_rate / 100, 4) if batch_pass_rate > 0 else 0
            
            # ä¸ºæ‰¹æ¬¡ä¸­çš„æ¯ä¸ªç­ç»„åˆ›å»ºè®°å½•ï¼ˆåªå–æœ€ç»ˆå·¥åºçš„ç­ç»„ï¼‰
            final_op_records = batch_group[batch_group['å·¥åºç¼–å·_æ•°å­—'] == final_operations['å·¥åºç¼–å·_æ•°å­—']]
            for _, final_op in final_op_records.iterrows():
                batch_record = final_op.copy()
                batch_record['æ‰¹æ¬¡åˆæ ¼ç‡'] = batch_pass_rate_percentage
                batch_record['æ‰¹æ¬¡æ€»ä¸åˆæ ¼æ•°'] = total_unqualified
                batch_record['æœ€ç»ˆå·¥åºåˆæ ¼æ•°'] = final_qualified
                batch_record['è®¡ç®—æ–¹å¼'] = 'æ‰¹æ¬¡çº§çœŸå®åˆæ ¼ç‡'
                batch_results.append(batch_record)
                
        except Exception as e:
            logging.warning(f"è·³è¿‡æ‰¹æ¬¡ {batch_no}-{product_no}ï¼Œè®¡ç®—å¤±è´¥: {e}")
            skipped_batches += 1
            continue
    
    if skipped_batches > 0:
        logging.warning(f"è·³è¿‡äº† {skipped_batches} ä¸ªæ‰¹æ¬¡ï¼Œå› ä¸ºæ•°æ®è´¨é‡é—®é¢˜")
    
    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
    batch_df = pd.DataFrame(batch_results)
    
    if batch_df.empty:
        logging.error("æ‰¹æ¬¡çº§è®¡ç®—ç»“æœä¸ºç©º")
        return pd.DataFrame()
    
    # é‡å‘½ååˆæ ¼ç‡ä¸ºå·¥åºåˆæ ¼ç‡ï¼ˆä¿æŒåŸæœ‰å­—æ®µï¼‰
    batch_df = batch_df.rename(columns={'åˆæ ¼ç‡': 'å·¥åºåˆæ ¼ç‡'})
    
    # ç§»é™¤ä¸éœ€è¦çš„åˆ—
    columns_to_remove = ['ç­ç»„æ’å', 'æ•°æ®è´¨é‡', 'äº§å“åºå·æ•°é‡', 'å·¥åºç¼–å·_æ•°å­—']
    batch_df = batch_df.drop(columns=columns_to_remove, errors='ignore')
    
    logging.info(f"æ‰¹æ¬¡çº§åˆæ ¼ç‡è®¡ç®—å®Œæˆ: {len(operation_agg)} â†’ {len(batch_df)} æ¡è®°å½•")
    logging.info(f"å¹³å‡æ‰¹æ¬¡åˆæ ¼ç‡: {batch_df['æ‰¹æ¬¡åˆæ ¼ç‡'].mean():.4f} (ç™¾åˆ†æ¯”æ ¼å¼)")
    
    return batch_df

def aggregate_team_passrate_data(df: pd.DataFrame, config: dict) -> pd.DataFrame:
    """
    æŒ‰æ‰¹æ¬¡ã€äº§å“å·ã€å·¥åºç¼–å·ã€å·¥åºåç§°ã€ç­ç»„åˆ†ç»„èšåˆæ•°æ®
    ç»Ÿä¸€å¤„ç†ä¸¤ç§æ£€éªŒç»“æœæ ¼å¼ï¼Œè®¡ç®—åˆæ ¼æ•°é‡å’Œåˆæ ¼ç‡
    """
    if df.empty:
        return df
    
    result = df.copy()
    logging.info("å¼€å§‹æŒ‰ä¸šåŠ¡ç»´åº¦åˆ†ç»„èšåˆæ•°æ®...")
    
    # æ£€æŸ¥å¿…è¦çš„åˆ†ç»„å­—æ®µ
    group_fields = ['æ‰¹æ¬¡å·', 'äº§å“å·', 'å·¥åºç¼–å·', 'å·¥åºåç§°', 'ç­ç»„']
    missing_fields = [field for field in group_fields if field not in result.columns]
    
    if missing_fields:
        logging.error(f"ç¼ºå°‘åˆ†ç»„å­—æ®µ: {missing_fields}")
        return pd.DataFrame()
    
    # å¤„ç†ä¸¤ç§æ£€éªŒç»“æœæ ¼å¼
    if 'æ£€éªŒç»“æœ' in result.columns and ('åˆæ ¼æ•°' not in result.columns or 'ä¸åˆæ ¼æ•°' not in result.columns):
        # æ ¼å¼1: ä¸ªäººäº§å“çº§æ£€éªŒç»“æœï¼Œéœ€è¦èšåˆè®¡ç®—
        logging.info("æ£€æµ‹åˆ°ä¸ªäººäº§å“çº§æ£€éªŒç»“æœï¼Œå¼€å§‹èšåˆè®¡ç®—...")
        
        # æ ‡å‡†åŒ–æ£€éªŒç»“æœ
        result['æ£€éªŒç»“æœ'] = result['æ£€éªŒç»“æœ'].str.strip().str.upper()
        result['æ˜¯å¦åˆæ ¼'] = result['æ£€éªŒç»“æœ'].isin(['åˆæ ¼', 'PASS'])
        
        # æŒ‰åˆ†ç»„å­—æ®µèšåˆ
        aggregated = result.groupby(group_fields).agg(
            åˆæ ¼æ•°=('æ˜¯å¦åˆæ ¼', 'sum'),
            ä¸åˆæ ¼æ•°=('æ˜¯å¦åˆæ ¼', lambda x: len(x) - x.sum()),
            æ€»æ£€éªŒæ•°=('æ˜¯å¦åˆæ ¼', 'count'),
            äº§å“åºå·æ•°é‡=('äº§å“åºå·', 'count'),
            file_mod_time=('file_mod_time', 'max'),
            source_file=('source_file', lambda x: ', '.join(set(x.dropna().astype(str).str.strip())))
        ).reset_index()
        
    elif 'åˆæ ¼æ•°' in result.columns and 'ä¸åˆæ ¼æ•°' in result.columns:
        # æ ¼å¼2: å·²æœ‰æ‰¹æ¬¡çº§ç»Ÿè®¡æ•°æ®ï¼Œç›´æ¥èšåˆ
        logging.info("æ£€æµ‹åˆ°æ‰¹æ¬¡çº§ç»Ÿè®¡æ•°æ®ï¼Œç›´æ¥èšåˆ...")
        
        # ç¡®ä¿æ•°å€¼ç±»å‹
        result['åˆæ ¼æ•°'] = pd.to_numeric(result['åˆæ ¼æ•°'], errors='coerce').fillna(0)
        result['ä¸åˆæ ¼æ•°'] = pd.to_numeric(result['ä¸åˆæ ¼æ•°'], errors='coerce').fillna(0)
        
        # æŒ‰åˆ†ç»„å­—æ®µèšåˆ
        aggregated = result.groupby(group_fields).agg(
            åˆæ ¼æ•°=('åˆæ ¼æ•°', 'sum'),
            ä¸åˆæ ¼æ•°=('ä¸åˆæ ¼æ•°', 'sum'),
            äº§å“åºå·æ•°é‡=('äº§å“åºå·', 'count'),
            file_mod_time=('file_mod_time', 'max'),
            source_file=('source_file', lambda x: ', '.join(set(x.dropna().astype(str).str.strip())))
        ).reset_index()
        
    else:
        logging.error("æ— æ³•è¯†åˆ«æ£€éªŒç»“æœæ ¼å¼")
        return pd.DataFrame()
    
    # è®¡ç®—åˆæ ¼ç‡ï¼ˆç™¾åˆ†æ¯”æ ¼å¼ï¼‰
    aggregated['æ€»æ£€éªŒæ•°'] = aggregated['åˆæ ¼æ•°'] + aggregated['ä¸åˆæ ¼æ•°']
    aggregated['åˆæ ¼ç‡'] = (aggregated['åˆæ ¼æ•°'] / aggregated['æ€»æ£€éªŒæ•°'] * 100).round(2)
    
    # ç§»é™¤æ— æ•ˆæ•°æ®ï¼ˆæ€»æ£€éªŒæ•°ä¸º0çš„è®°å½•ï¼‰
    valid_data = aggregated[aggregated['æ€»æ£€éªŒæ•°'] > 0].copy()
    
    # æ·»åŠ æ•°æ®è´¨é‡æ ‡è®°
    valid_data['æ•°æ®è´¨é‡'] = 'æ­£å¸¸'
    
    # è®¡ç®—ç­ç»„æ’åï¼ˆæŒ‰åˆæ ¼ç‡ï¼‰
    valid_data = valid_data.sort_values('åˆæ ¼ç‡', ascending=False)
    valid_data['ç­ç»„æ’å'] = valid_data.groupby(['æ‰¹æ¬¡å·', 'äº§å“å·', 'å·¥åºç¼–å·', 'å·¥åºåç§°']).cumcount() + 1
    
    logging.info(f"èšåˆå®Œæˆ: {len(result)} â†’ {len(valid_data)} æ¡è®°å½•")
    logging.info(f"å¹³å‡åˆæ ¼ç‡: {valid_data['åˆæ ¼ç‡'].mean():.2f}%")
    
    return valid_data

def normalize_batch_and_operation_for_matching(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ ‡å‡†åŒ–æ‰¹æ¬¡å·å’Œå·¥åºç¼–å·ä»¥åŒ¹é…SFCæ‰¹æ¬¡æŠ¥å·¥è¡¨
    """
    result = df.copy()
    
    # æ ‡å‡†åŒ–æ‰¹æ¬¡å·ï¼šç§»é™¤IPQCKå‰ç¼€
    if 'æ‰¹æ¬¡å·' in result.columns:
        result['æ‰¹æ¬¡å·_æ ‡å‡†åŒ–'] = result['æ‰¹æ¬¡å·'].astype(str).str.replace(r'^IPQCK', '', regex=True)
    
    # æ ‡å‡†åŒ–å·¥åºç¼–å·ï¼šæ•°å­—è¡¥é›¶åˆ°4ä½
    if 'å·¥åºç¼–å·' in result.columns:
        result['å·¥åºç¼–å·_æ ‡å‡†åŒ–'] = result['å·¥åºç¼–å·'].astype(str).str.strip()
        # åªå¯¹çº¯æ•°å­—è¿›è¡Œè¡¥é›¶åˆ°4ä½
        mask = result['å·¥åºç¼–å·_æ ‡å‡†åŒ–'].str.match(r'^\d+$')
        result.loc[mask, 'å·¥åºç¼–å·_æ ‡å‡†åŒ–'] = result.loc[mask, 'å·¥åºç¼–å·_æ ‡å‡†åŒ–'].str.zfill(4)
        logging.info("å·²æ ‡å‡†åŒ–å·¥åºç¼–å·ï¼šæ•°å­—è¡¥é›¶åˆ°4ä½ï¼ˆä¸SFCæ‰¹æ¬¡æŠ¥å·¥è¡¨ä¿æŒä¸€è‡´ï¼‰")
    
    return result

def enrich_with_processing_time(df: pd.DataFrame, config: dict) -> pd.DataFrame:
    """
    ä»SFCæ‰¹æ¬¡æŠ¥å·¥è¡¨è·å–å·¥åºåŠ å·¥æ—¶é—´ï¼Œå¢å¼ºç­ç»„åˆæ ¼ç‡æ•°æ®
    """
    if df.empty:
        return df
    
    logging.info("å¼€å§‹ä»SFCæ‰¹æ¬¡æŠ¥å·¥è¡¨è·å–å·¥åºåŠ å·¥æ—¶é—´...")
    
    # è¯»å–SFCæ‰¹æ¬¡æŠ¥å·¥è¡¨
    output_dir = config.get("output", {}).get("base_dir", "publish")
    if not os.path.isabs(output_dir):
        output_dir = os.path.join(os.path.dirname(__file__), output_dir)
    
    sfc_batch_file = os.path.join(output_dir, "SFC_batch_report_latest.parquet")
    
    if not os.path.exists(sfc_batch_file):
        logging.warning(f"SFCæ‰¹æ¬¡æŠ¥å·¥è¡¨ä¸å­˜åœ¨: {sfc_batch_file}")
        return df
    
    try:
        # è¯»å–SFCæ‰¹æ¬¡æŠ¥å·¥è¡¨
        sfc_batch_df = pd.read_parquet(sfc_batch_file)
        logging.info(f"æˆåŠŸè¯»å–SFCæ‰¹æ¬¡æŠ¥å·¥è¡¨: {len(sfc_batch_df)} æ¡è®°å½•")
        
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        required_fields = ['BatchNumber', 'Operation', 'Operation description', 'TrackOutTime']
        missing_fields = [field for field in required_fields if field not in sfc_batch_df.columns]
        
        if missing_fields:
            logging.error(f"SFCæ‰¹æ¬¡æŠ¥å·¥è¡¨ç¼ºå°‘å­—æ®µ: {missing_fields}")
            return df
        
        # æå–éœ€è¦çš„å­—æ®µå¹¶å»é‡ï¼ˆä¸€ä¸ªæ‰¹æ¬¡+å·¥åºå¯èƒ½æœ‰å¤šä¸ªè®°å½•ï¼Œå–æœ€æ–°çš„TrackOutTimeï¼‰
        time_data = sfc_batch_df[required_fields].copy()
        
        # æŒ‰æ‰¹æ¬¡+å·¥åºåˆ†ç»„ï¼Œå–æœ€æ–°çš„TrackOutTime
        time_data_sorted = time_data.sort_values('TrackOutTime', ascending=False)
        time_data_unique = time_data_sorted.drop_duplicates(
            subset=['BatchNumber', 'Operation'], 
            keep='first'
        )
        
        logging.info(f"SFCæ‰¹æ¬¡æŠ¥å·¥è¡¨å»é‡å: {len(time_data_unique)} æ¡å”¯ä¸€è®°å½•")
        
        # æ‰§è¡Œæ•°æ®å¢å¼º - å·¦è¿æ¥
        result = df.copy()
        
        # æ ‡å‡†åŒ–æ•°æ®æ ¼å¼ä»¥æå‡åŒ¹é…ç‡
        result_normalized = normalize_batch_and_operation_for_matching(result)
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥æ ‡å‡†åŒ–æ•ˆæœ
        logging.info(f"æ•°æ®æ ‡å‡†åŒ–æ•ˆæœ:")
        if 'æ‰¹æ¬¡å·' in result.columns:
            original_batches = result['æ‰¹æ¬¡å·'].dropna().unique()[:5]
            normalized_batches = result_normalized['æ‰¹æ¬¡å·_æ ‡å‡†åŒ–'].dropna().unique()[:5]
            logging.info(f"  æ‰¹æ¬¡å·æ ‡å‡†åŒ–: {list(original_batches)} -> {list(normalized_batches)}")
        
        if 'å·¥åºç¼–å·' in result.columns:
            original_ops = result['å·¥åºç¼–å·'].dropna().unique()[:5]
            normalized_ops = result_normalized['å·¥åºç¼–å·_æ ‡å‡†åŒ–'].dropna().unique()[:5]
            logging.info(f"  å·¥åºç¼–å·æ ‡å‡†åŒ–: {list(original_ops)} -> {list(normalized_ops)}")
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥è¾“å…¥æ•°æ®çŠ¶æ€
        logging.info(f"ç­ç»„åˆæ ¼ç‡èšåˆæ•°æ®è°ƒè¯•ä¿¡æ¯:")
        logging.info(f"  æ•°æ®å½¢çŠ¶: {result_normalized.shape}")
        logging.info(f"  ç´¢å¼•ç±»å‹: {type(result_normalized.index)}")
        logging.info(f"  å…³é”®å­—æ®µå­˜åœ¨: æ‰¹æ¬¡å·_æ ‡å‡†åŒ–={('æ‰¹æ¬¡å·_æ ‡å‡†åŒ–' in result_normalized.columns)}, å·¥åºç¼–å·_æ ‡å‡†åŒ–={('å·¥åºç¼–å·_æ ‡å‡†åŒ–' in result_normalized.columns)}")
        logging.info(f"  å…³é”®å­—æ®µæ ·æœ¬: æ‰¹æ¬¡å·_æ ‡å‡†åŒ–={result_normalized['æ‰¹æ¬¡å·_æ ‡å‡†åŒ–'].iloc[0] if len(result_normalized) > 0 else 'N/A'}, å·¥åºç¼–å·_æ ‡å‡†åŒ–={result_normalized['å·¥åºç¼–å·_æ ‡å‡†åŒ–'].iloc[0] if len(result_normalized) > 0 else 'N/A'}")
        
        logging.info(f"SFCæ‰¹æ¬¡æŠ¥å·¥è¡¨è°ƒè¯•ä¿¡æ¯:")
        logging.info(f"  æ•°æ®å½¢çŠ¶: {time_data_unique.shape}")
        logging.info(f"  ç´¢å¼•ç±»å‹: {type(time_data_unique.index)}")
        logging.info(f"  å…³é”®å­—æ®µæ ·æœ¬: BatchNumber={time_data_unique['BatchNumber'].iloc[0] if len(time_data_unique) > 0 else 'N/A'}, Operation={time_data_unique['Operation'].iloc[0] if len(time_data_unique) > 0 else 'N/A'}")
        
        # æ‰§è¡Œè¿æ¥ - ä½¿ç”¨æ ‡å‡†åŒ–åçš„å­—æ®µ
        try:
            logging.info("å¼€å§‹æ‰§è¡Œåˆå¹¶æ“ä½œï¼ˆä½¿ç”¨æ ‡å‡†åŒ–å­—æ®µï¼‰...")
            merged = result_normalized.merge(
                time_data_unique[['BatchNumber', 'Operation', 'TrackOutTime']],
                left_on=['æ‰¹æ¬¡å·_æ ‡å‡†åŒ–', 'å·¥åºç¼–å·_æ ‡å‡†åŒ–'],
                right_on=['BatchNumber', 'Operation'],
                how='left'
            )
            logging.info("åˆå¹¶æ“ä½œæˆåŠŸå®Œæˆ")
        except Exception as merge_error:
            logging.error(f"åˆå¹¶æ“ä½œå¤±è´¥: {merge_error}")
            logging.error(f"åˆå¹¶é”™è¯¯ç±»å‹: {type(merge_error)}")
            # å°è¯•æ›´ç®€å•çš„åˆå¹¶æ–¹å¼
            try:
                logging.info("å°è¯•ç®€åŒ–åˆå¹¶æ–¹å¼...")
                merged = result_normalized.merge(
                    time_data_unique[['BatchNumber', 'Operation', 'TrackOutTime']].reset_index(drop=True),
                    left_on=['æ‰¹æ¬¡å·_æ ‡å‡†åŒ–', 'å·¥åºç¼–å·_æ ‡å‡†åŒ–'],
                    right_on=['BatchNumber', 'Operation'],
                    how='left'
                )
                logging.info("ç®€åŒ–åˆå¹¶æˆåŠŸ")
            except Exception as simple_error:
                logging.error(f"ç®€åŒ–åˆå¹¶ä¹Ÿå¤±è´¥: {simple_error}")
                raise merge_error
        
        # é‡å‘½åTrackOutTimeä¸ºå·¥åºåŠ å·¥æ—¶é—´
        merged = merged.rename(columns={'TrackOutTime': 'å·¥åºåŠ å·¥æ—¶é—´'})
        
        # ç§»é™¤é‡å¤çš„è¿æ¥å­—æ®µ
        merged = merged.drop(columns=['BatchNumber', 'Operation'], errors='ignore')
        
        # æ›´æ–°å·¥åºç¼–å·ä¸ºæ ‡å‡†åŒ–æ ¼å¼ï¼ˆä¿æŒä¸SFCæ‰¹æ¬¡æŠ¥å·¥è¡¨ä¸€è‡´ï¼‰
        if 'å·¥åºç¼–å·_æ ‡å‡†åŒ–' in merged.columns:
            merged['å·¥åºç¼–å·'] = merged['å·¥åºç¼–å·_æ ‡å‡†åŒ–']
            merged = merged.drop(columns=['å·¥åºç¼–å·_æ ‡å‡†åŒ–'], errors='ignore')
            logging.info("å·²æ›´æ–°å·¥åºç¼–å·ä¸º4ä½æ ‡å‡†åŒ–æ ¼å¼")
        
        # ç»Ÿè®¡åŒ¹é…ç»“æœ
        matched_count = merged['å·¥åºåŠ å·¥æ—¶é—´'].notna().sum()
        total_count = len(merged)
        match_rate = matched_count / total_count * 100 if total_count > 0 else 0
        
        logging.info(f"æ•°æ®å¢å¼ºå®Œæˆ: {matched_count}/{total_count} æ¡è®°å½•åŒ¹é…åˆ°åŠ å·¥æ—¶é—´ ({match_rate:.1f}%)")
        
        # å¦‚æœåŒ¹é…ç‡è¾ƒä½ï¼Œç»™å‡ºè­¦å‘Š
        if match_rate < 80:
            logging.warning(f"åŠ å·¥æ—¶é—´åŒ¹é…ç‡è¾ƒä½ ({match_rate:.1f}%)ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®ä¸ä¸€è‡´")
        
        return merged
        
    except Exception as e:
        logging.error(f"æ•°æ®å¢å¼ºå¤±è´¥: {e}")
        return df

def calculate_team_passrate_metrics(df: pd.DataFrame, config: dict) -> pd.DataFrame:
    """
    è¡¥å……èšåˆæ•°æ®ä¸­ç¼ºå¤±çš„æŒ‡æ ‡å­—æ®µ
    æ³¨æ„ï¼šå¤§éƒ¨åˆ†æŒ‡æ ‡å·²åœ¨aggregate_team_passrate_dataä¸­è®¡ç®—
    """
    logging.info("å¼€å§‹è¡¥å……èšåˆæ•°æ®çš„æŒ‡æ ‡å­—æ®µ...")
    
    result = df.copy()
    
    # æ·»åŠ å¤„ç†æ—¶é—´æˆ³ï¼ˆèšåˆæ•°æ®ä¸­ç¼ºå¤±ï¼‰
    result['å¤„ç†æ—¶é—´'] = datetime.now()
    
    # ç¡®ä¿æ•°æ®è´¨é‡æ ‡è¯†ï¼ˆå¦‚æœèšåˆå‡½æ•°æœªè®¾ç½®ï¼‰
    if 'æ•°æ®è´¨é‡' not in result.columns:
        result['æ•°æ®è´¨é‡'] = 'æ­£å¸¸'
    
    # æ ‡è®°å¼‚å¸¸æ•°æ®ï¼ˆåŸºäºåˆæ ¼ç‡èŒƒå›´ï¼‰
    if 'åˆæ ¼ç‡' in result.columns:
        result.loc[result['åˆæ ¼ç‡'] < 0, 'æ•°æ®è´¨é‡'] = 'å¼‚å¸¸'
        result.loc[result['åˆæ ¼ç‡'] > 100, 'æ•°æ®è´¨é‡'] = 'å¼‚å¸¸'
    
    logging.info("èšåˆæ•°æ®æŒ‡æ ‡è¡¥å……å®Œæˆ")
    return result

def process_team_passrate_data(config: dict, force_full_refresh: bool = False) -> pd.DataFrame:
    """
    å¤„ç†ç­ç»„åˆæ ¼ç‡æ•°æ®çš„ä¸»å‡½æ•°
    """
    logging.info("=" * 60)
    logging.info("å¼€å§‹å¤„ç†SFCç­ç»„åˆæ ¼ç‡æ•°æ®")
    logging.info("=" * 60)
    
    try:
        # 1. è¯»å–æ•°æ®
        raw_df = read_team_passrate_data(config)
        
        # 2. éªŒè¯æ•°æ®
        if not validate_team_passrate_data(raw_df, config):
            raise ValueError("ç­ç»„åˆæ ¼ç‡æ•°æ®éªŒè¯å¤±è´¥")
        
        # 3. æ¸…æ´—æ•°æ®
        cleaned_df = clean_team_passrate_data(raw_df, config)
        
        # 4. æŒ‰æ‰¹æ¬¡è®¡ç®—çœŸå®åˆæ ¼ç‡ï¼ˆè€ƒè™‘ç´¯ç§¯ç¼ºé™·ï¼‰
        aggregated_df = aggregate_team_passrate_data_batch_level(cleaned_df, config)
        
        # 5. æ•°æ®å¢å¼ºï¼šä»SFCæ‰¹æ¬¡æŠ¥å·¥è¡¨è·å–å·¥åºåŠ å·¥æ—¶é—´
        enriched_df = enrich_with_processing_time(aggregated_df, config)
        
        # 6. è®¡ç®—æŒ‡æ ‡ï¼ˆèšåˆåçš„æ•°æ®å·²åŒ…å«åˆæ ¼ç‡ï¼Œæ­¤æ­¥éª¤å¯ç®€åŒ–ï¼‰
        result_df = calculate_team_passrate_metrics(enriched_df, config)
        
        # 7. æœ€ç»ˆå»é‡ï¼šå¤„ç†æ‰¹æ¬¡çº§èšåˆåçš„é‡å¤æ•°æ®
        logging.info("å¼€å§‹æœ€ç»ˆå»é‡å¤„ç†...")
        before_final_dedup = len(result_df)
        
        # æŒ‰å®Œæ•´ä¸šåŠ¡é”®å»é‡ï¼Œä¿ç•™æœ€æ–°æ–‡ä»¶æ—¶é—´æˆ³çš„è®°å½•
        final_business_keys = ['æ‰¹æ¬¡å·', 'äº§å“åºå·', 'å·¥åºç¼–å·', 'å·¥åºåç§°']
        missing_final_keys = [key for key in final_business_keys if key not in result_df.columns]
        
        if missing_final_keys:
            logging.warning(f"æœ€ç»ˆå»é‡ç¼ºå°‘ä¸šåŠ¡é”®å­—æ®µ {missing_final_keys}")
            # ä½¿ç”¨å¯ç”¨å­—æ®µå»é‡
            available_keys = [key for key in final_business_keys if key in result_df.columns]
            if available_keys:
                result_df = result_df.drop_duplicates(subset=available_keys, keep='first')
                logging.info(f"æœ€ç»ˆå»é‡: åŸºäº {available_keys} ç§»é™¤é‡å¤è®°å½•")
        else:
            # æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ’åºï¼Œä¿ç•™æœ€æ–°çš„è®°å½•
            result_df = result_df.sort_values('file_mod_time', ascending=False)
            result_df = result_df.drop_duplicates(subset=final_business_keys, keep='first')
            
        final_dedup_count = before_final_dedup - len(result_df)
        logging.info(f"æœ€ç»ˆå»é‡å®Œæˆ: ç§»é™¤ {final_dedup_count} æ¡é‡å¤è®°å½•")
        
        # 8. æ•°æ®è´¨é‡æ£€æŸ¥
        logging.info(f"æœ€ç»ˆæ•°æ®ç»Ÿè®¡:")
        logging.info(f"  æ€»è®°å½•æ•°: {len(result_df)}")
        logging.info(f"  ç­ç»„æ•°: {result_df['ç­ç»„'].nunique()}")
        logging.info(f"  æ‰¹æ¬¡æ•°: {result_df['æ‰¹æ¬¡å·'].nunique()}")
        
        if 'äº§å“åˆæ ¼ç‡' in result_df.columns:
            avg_pass_rate = result_df['äº§å“åˆæ ¼ç‡'].mean()
            logging.info(f"  å¹³å‡åˆæ ¼ç‡: {avg_pass_rate:.2f}%")
        
        return result_df
        
    except Exception as e:
        logging.error(f"ç­ç»„åˆæ ¼ç‡æ•°æ®å¤„ç†å¤±è´¥: {e}")
        raise

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='SFCç­ç»„åˆæ ¼ç‡æ•°æ®æ¸…æ´—ETLè„šæœ¬')
    parser.add_argument('--config', type=str, 
                       default="../03_é…ç½®æ–‡ä»¶/config/config_sfc_team_passrate.yaml",
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--mode', choices=['incremental', 'full'], 
                       default='full', help='åˆ·æ–°æ¨¡å¼')
    parser.add_argument('--unattended', action='store_true', 
                       help='æ— äººå€¼å®ˆæ¨¡å¼ï¼Œä¸è¿›è¡Œäº¤äº’å¼æç¤º')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config_path = os.path.join(os.path.dirname(__file__), args.config)
    config = load_config(config_path)
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(config, "sfc_team_passrate")
    
    # å¤„ç†æ•°æ®
    try:
        result_df = process_team_passrate_data(config, force_full_refresh=(args.mode == 'full'))
        
        if result_df.empty:
            logging.warning("ç­ç»„åˆæ ¼ç‡å¤„ç†åçš„æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
        else:
            # ä¿å­˜ç»“æœ
            output_dir = config.get("output", {}).get("base_dir", "publish")
            output_dir = os.path.join(os.path.dirname(__file__), output_dir) if not os.path.isabs(output_dir) else output_dir
            os.makedirs(output_dir, exist_ok=True)
            
            # ä¿å­˜åˆ°latestæ–‡ä»¶
            latest_file = os.path.join(output_dir, "SFC_Team_PassRate_latest.parquet")
            save_to_parquet(result_df, latest_file, config)
            logging.info(f"ç­ç»„åˆæ ¼ç‡æ•°æ®å·²ä¿å­˜: {latest_file}")
            
            # ä¿å­˜ExceléªŒè¯æ–‡ä»¶ï¼ˆå‰1000è¡Œï¼‰
            excel_dir = os.path.join(output_dir, "excel")
            os.makedirs(excel_dir, exist_ok=True)
            excel_file = os.path.join(excel_dir, "SFC_Team_PassRate_latest.xlsx")
            result_df.head(1000).to_excel(excel_file, index=False)
            logging.info(f"ç­ç»„åˆæ ¼ç‡ExceléªŒè¯æ–‡ä»¶å·²ä¿å­˜: {excel_file}")
        
        logging.info("SFCç­ç»„åˆæ ¼ç‡æ•°æ®å¤„ç†å®Œæˆ")
        
    except Exception as e:
        logging.exception(f"ETLå¤„ç†å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
