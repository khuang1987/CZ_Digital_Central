"""
é‡å¤æ•°æ®æ£€æµ‹è„šæœ¬
åˆ†æè¾“å‡ºæ•°æ®ä¸­çš„é‡å¤æ¨¡å¼ï¼Œæ‰¾å‡ºå»é‡é€»è¾‘å¤±æ•ˆçš„åŸå› 
"""

import pandas as pd
import numpy as np

def detect_duplicates():
    """æ£€æµ‹è¾“å‡ºæ•°æ®ä¸­çš„é‡å¤æ¨¡å¼"""
    
    print("=" * 80)
    print("é‡å¤æ•°æ®æ£€æµ‹åˆ†æ")
    print("=" * 80)
    
    # 1. è¯»å–å¤„ç†åçš„æ•°æ®æ–‡ä»¶
    output_file = r"C:/Users/huangk14/OneDrive - Medtronic PLC/CZ Production - æ–‡æ¡£/General/POWER BI æ•°æ®æº V2/30-MESå¯¼å‡ºæ•°æ®/publish/SFC_Team_PassRate_latest.parquet"
    
    try:
        df = pd.read_parquet(output_file)
        print(f"âœ… æˆåŠŸè¯»å–è¾“å‡ºæ–‡ä»¶: {len(df)} æ¡è®°å½•")
    except Exception as e:
        print(f"âŒ è¯»å–è¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # 2. æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
    print(f"\nğŸ“Š æ•°æ®åŸºæœ¬ä¿¡æ¯:")
    print(f"  æ€»è®°å½•æ•°: {len(df)}")
    print(f"  æ‰¹æ¬¡æ•°: {df['æ‰¹æ¬¡å·'].nunique()}")
    print(f"  ç­ç»„æ•°: {df['ç­ç»„'].nunique()}")
    print(f"  å·¥åºæ•°: {df['å·¥åºç¼–å·'].nunique()}")
    
    # 3. æ£€æŸ¥åŸºäºå½“å‰ä¸šåŠ¡é”®çš„é‡å¤
    print(f"\nğŸ” æ£€æŸ¥å½“å‰ä¸šåŠ¡é”®é‡å¤æƒ…å†µ:")
    business_keys = ['æ‰¹æ¬¡å·', 'äº§å“åºå·', 'å·¥åºç¼–å·', 'å·¥åºåç§°']
    
    # æ£€æŸ¥ä¸šåŠ¡é”®æ˜¯å¦å­˜åœ¨
    missing_keys = [key for key in business_keys if key not in df.columns]
    if missing_keys:
        print(f"âŒ ç¼ºå°‘ä¸šåŠ¡é”®å­—æ®µ: {missing_keys}")
        business_keys = [key for key in business_keys if key in df.columns]
    
    print(f"  ä½¿ç”¨çš„ä¸šåŠ¡é”®: {business_keys}")
    
    # ç»Ÿè®¡ä¸šåŠ¡é”®é‡å¤
    duplicate_groups = df.groupby(business_keys).size()
    duplicates = duplicate_groups[duplicate_groups > 1]
    
    if len(duplicates) == 0:
        print(f"âœ… åŸºäºä¸šåŠ¡é”® {business_keys} æ— é‡å¤æ•°æ®")
    else:
        print(f"âŒ å‘ç° {len(duplicates)} ç»„é‡å¤æ•°æ®:")
        print(f"  é‡å¤è®°å½•æ€»æ•°: {duplicates.sum()}")
        print(f"  é‡å¤ç»„æ•°: {len(duplicates)}")
        
        # æ˜¾ç¤ºå‰5ç»„é‡å¤æ•°æ®
        print(f"\nğŸ“‹ é‡å¤æ•°æ®ç¤ºä¾‹ (å‰5ç»„):")
        print("-" * 100)
        for i, (idx, count) in enumerate(duplicates.head().items()):
            if isinstance(idx, tuple):
                key_str = " | ".join([str(x) for x in idx])
            else:
                key_str = str(idx)
            print(f"  {i+1}. {key_str} -> {count} æ¡è®°å½•")
            
            # æ˜¾ç¤ºè¯¥ç»„çš„è¯¦ç»†æ•°æ®
            if len(business_keys) == 4:
                batch_num, product_num, operation_num, operation_name = idx
                group_data = df[
                    (df['æ‰¹æ¬¡å·'] == batch_num) & 
                    (df['äº§å“åºå·'] == product_num) & 
                    (df['å·¥åºç¼–å·'] == operation_num) & 
                    (df['å·¥åºåç§°'] == operation_name)
                ]
            else:
                # å¤„ç†å­—æ®µä¸å­˜åœ¨çš„æƒ…å†µ
                group_data = df[df[business_keys[0]] == idx] if len(business_keys) == 1 else pd.DataFrame()
            
            if not group_data.empty:
                print(f"     è¯¦ç»†è®°å½•:")
                for _, row in group_data.iterrows():
                    print(f"       ç­ç»„: {row.get('ç­ç»„', 'N/A')}, åˆæ ¼æ•°: {row.get('åˆæ ¼æ•°', 'N/A')}, ä¸åˆæ ¼æ•°: {row.get('ä¸åˆæ ¼æ•°', 'N/A')}, æ‰¹æ¬¡åˆæ ¼ç‡: {row.get('æ‰¹æ¬¡åˆæ ¼ç‡', 'N/A')}")
            print()
    
    # 4. æ£€æŸ¥ä¸åŒç»´åº¦çš„é‡å¤
    print(f"\nğŸ” æ£€æŸ¥ä¸åŒç»´åº¦çš„é‡å¤æƒ…å†µ:")
    
    # æ£€æŸ¥æ‰¹æ¬¡+å·¥åºé‡å¤
    batch_op_duplicates = df.groupby(['æ‰¹æ¬¡å·', 'å·¥åºç¼–å·']).size()
    batch_op_dup_count = (batch_op_duplicates > 1).sum()
    print(f"  æ‰¹æ¬¡+å·¥åºé‡å¤ç»„æ•°: {batch_op_dup_count}")
    
    # æ£€æŸ¥æ‰¹æ¬¡é‡å¤
    batch_duplicates = df.groupby('æ‰¹æ¬¡å·').size()
    batch_dup_count = (batch_duplicates > 1).sum()
    print(f"  æ‰¹æ¬¡é‡å¤ç»„æ•°: {batch_dup_count}")
    
    # 5. åˆ†æé‡å¤åŸå› 
    if len(duplicates) > 0:
        print(f"\nğŸ”¬ é‡å¤åŸå› åˆ†æ:")
        
        # å–ä¸€ä¸ªé‡å¤ç»„è¿›è¡Œè¯¦ç»†åˆ†æ
        sample_duplicate_key = duplicates.index[0]
        sample_group = pd.DataFrame()
        
        if isinstance(sample_duplicate_key, tuple):
            if len(business_keys) == 3:  # å½“å‰å®é™…ä½¿ç”¨çš„ä¸šåŠ¡é”®
                batch_num, operation_num, operation_name = sample_duplicate_key
                sample_group = df[
                    (df['æ‰¹æ¬¡å·'] == batch_num) & 
                    (df['å·¥åºç¼–å·'] == operation_num) & 
                    (df['å·¥åºåç§°'] == operation_name)
                ]
        else:
            sample_group = df[df[business_keys[0]] == sample_duplicate_key]
        
        if sample_group.empty:
            print(f"  âŒ æ— æ³•è·å–æ ·æœ¬é‡å¤ç»„æ•°æ®")
            return
        
        print(f"  æ ·æœ¬é‡å¤ç»„è¯¦ç»†å­—æ®µå¯¹æ¯”:")
        print("-" * 80)
        
        # æ˜¾ç¤ºæ‰€æœ‰å­—æ®µçš„å€¼ï¼Œæ‰¾å‡ºå·®å¼‚
        for col in sample_group.columns:
            unique_values = sample_group[col].unique()
            if len(unique_values) > 1:
                print(f"    {col}: {unique_values}")
            else:
                print(f"    {col}: {unique_values[0]}")
        
        print(f"\nğŸ’¡ å¯èƒ½çš„é‡å¤åŸå› :")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç­ç»„ä¸åŒå¯¼è‡´çš„é‡å¤
        if 'ç­ç»„' in sample_group.columns and sample_group['ç­ç»„'].nunique() > 1:
            print(f"  âŒ åŒä¸€ä¸šåŠ¡é”®å­˜åœ¨å¤šä¸ªç­ç»„è®°å½•")
            print(f"  ğŸ’¡ å»ºè®®: å°† 'ç­ç»„' åŠ å…¥ä¸šåŠ¡é”®è¿›è¡Œå»é‡")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯äº§å“å·ä¸åŒå¯¼è‡´çš„é‡å¤
        if 'äº§å“å·' in sample_group.columns and sample_group['äº§å“å·'].nunique() > 1:
            print(f"  âŒ åŒä¸€æ‰¹æ¬¡å·¥åºå­˜åœ¨å¤šä¸ªäº§å“å·")
            print(f"  ğŸ’¡ å»ºè®®: æ£€æŸ¥æ˜¯å¦åº”è¯¥åŒ…å« 'äº§å“å·' åœ¨ä¸šåŠ¡é”®ä¸­")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ä»¶æ¥æºä¸åŒå¯¼è‡´çš„é‡å¤
        if 'source_file' in sample_group.columns and sample_group['source_file'].nunique() > 1:
            print(f"  âŒ åŒä¸€ä¸šåŠ¡é”®æ¥è‡ªå¤šä¸ªæºæ–‡ä»¶")
            print(f"  ğŸ’¡ å»ºè®®: æ£€æŸ¥æ–‡ä»¶è¯»å–é€»è¾‘")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ—¶é—´æˆ³ä¸åŒå¯¼è‡´çš„é‡å¤
        if 'file_mod_time' in sample_group.columns and sample_group['file_mod_time'].nunique() > 1:
            print(f"  âŒ åŒä¸€ä¸šåŠ¡é”®æœ‰å¤šä¸ªä¿®æ”¹æ—¶é—´")
            print(f"  ğŸ’¡ å»ºè®®: ç¡®è®¤æ—¶é—´æ’åºé€»è¾‘")
    
    # 6. å»é‡å»ºè®®
    print(f"\nğŸ› ï¸ å»é‡å»ºè®®:")
    
    if len(duplicates) > 0:
        print(f"  1. å½“å‰ä¸šåŠ¡é”®: {business_keys}")
        print(f"  2. å‘ç°é‡å¤: {len(duplicates)} ç»„")
        
        # å»ºè®®æ–°çš„ä¸šåŠ¡é”®
        suggested_keys = business_keys.copy()
        if 'ç­ç»„' in df.columns and 'ç­ç»„' not in suggested_keys:
            suggested_keys.append('ç­ç»„')
        
        print(f"  3. å»ºè®®ä¸šåŠ¡é”®: {suggested_keys}")
        
        # æµ‹è¯•æ–°ä¸šåŠ¡é”®çš„å»é‡æ•ˆæœ
        new_duplicate_groups = df.groupby(suggested_keys).size()
        new_duplicates = new_duplicate_groups[new_duplicate_groups > 1]
        
        print(f"  4. æ–°ä¸šåŠ¡é”®é‡å¤æ•°: {len(new_duplicates)} ç»„")
        
        if len(new_duplicates) == 0:
            print(f"  âœ… æ–°ä¸šåŠ¡é”®å¯ä»¥å®Œå…¨æ¶ˆé™¤é‡å¤")
        else:
            print(f"  âš ï¸ æ–°ä¸šåŠ¡é”®ä»æœ‰ {len(new_duplicates)} ç»„é‡å¤ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ")
    
    print(f"\n" + "=" * 80)
    print("æ£€æµ‹å®Œæˆ")
    print("=" * 80)

if __name__ == "__main__":
    detect_duplicates()
