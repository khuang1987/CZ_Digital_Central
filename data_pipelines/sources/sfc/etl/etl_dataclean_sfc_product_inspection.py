"""
SFCäº§å“æ£€éªŒè®°å½•æ•°æ®æ¸…æ´—ETLè„šæœ¬
åŠŸèƒ½ï¼šä»SharePointè¯»å–äº§å“æ£€éªŒè®°å½•Excelæ•°æ®ï¼Œè¿›è¡Œæ•°æ®å¤„ç†ã€å»é‡ã€å¢é‡å¤„ç†ï¼Œè¾“å‡ºä¸ºParquetæ ¼å¼ä¾›Power BIä½¿ç”¨
"""

import os
import sys
import time
import logging
import glob
import json
import hashlib
import threading
import argparse
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List, Set
import re
import warnings
warnings.filterwarnings('ignore')
from pathlib import Path

# Windowså¹³å°æ”¯æŒ
try:
    import msvcrt
    HAS_MSVCRT = True
except ImportError:
    HAS_MSVCRT = False

import pandas as pd
import numpy as np
import yaml

try:
    import pyarrow as pa
    import pyarrow.parquet as pq
except ImportError:
    print("è­¦å‘Šï¼šæœªå®‰è£…pyarrowï¼Œå°†æ— æ³•ä¿å­˜Parquetæ ¼å¼")
    pq = None


WRITE_PARQUET_OUTPUT = False

# é¡¹ç›®æ ¹ç›®å½•
project_root = str(Path(__file__).resolve().parents[4])

# å¯¼å…¥å…±äº«åŸºç¡€è®¾æ–½å·¥å…·å‡½æ•°
from shared_infrastructure.utils.etl_utils import (
    setup_logging,
    setup_logging_with_rotation,
    load_config,
    save_to_parquet,
    save_to_dual_locations,
    read_sharepoint_excel,
    IncrementalProcessor
)
from shared_infrastructure.utils.path_resolver import (
    get_config_path,
    get_log_path,
    get_state_path,
    get_path_resolver
)
from shared_infrastructure.utils.db_utils import get_default_db_manager

# è·å–è·¯å¾„è§£æå™¨
resolver = get_path_resolver()

# é…ç½®è·¯å¾„
CONFIG_PATH = get_config_path("sfc_product_inspection", "sfc", os.path.dirname(os.path.abspath(__file__)))
LOG_PATH = get_log_path("sfc")
STATE_PATH = get_state_path("sfc_product_inspection", "sfc", os.path.dirname(os.path.abspath(__file__)))


def extract_report_date_from_filename(filename: str) -> datetime:
    """
    ä»æ–‡ä»¶åæå–æŠ¥è¡¨æ—¥æœŸ
    IGPR-20251202080001 -> 2025-12-02 08:00:01
    """
    try:
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… IGPR-YYYYMMDDHHMMSS æ ¼å¼
        pattern = r'IGPR-(\d{4})(\d{2})(\d{2})(\d{2})(\d{2})(\d{2})'
        match = re.search(pattern, filename)
        
        if match:
            year, month, day, hour, minute, second = match.groups()
            return datetime(int(year), int(month), int(day), int(hour), int(minute), int(second))
        else:
            # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›æ–‡ä»¶ä¿®æ”¹æ—¶é—´ä½œä¸ºå¤‡é€‰
            logging.warning(f"æ— æ³•ä»æ–‡ä»¶åæå–æ—¥æœŸ: {filename}ï¼Œä½¿ç”¨å½“å‰æ—¶é—´")
            return datetime.now()
            
    except Exception as e:
        logging.warning(f"è§£ææ–‡ä»¶åæ—¥æœŸå¤±è´¥ {filename}: {e}")
        return datetime.now()


def generate_record_hash(row: pd.Series, key_fields: List[str]) -> str:
    """
    ç”Ÿæˆè®°å½•çš„å”¯ä¸€hashå€¼ï¼ˆåŸºäºä¸šåŠ¡å­—æ®µï¼Œä¸å«æ–‡ä»¶åï¼‰
    """
    try:
        # æ„å»ºå…³é”®å­—çš„å­—ç¬¦ä¸²è¡¨ç¤º
        key_values = []
        for field in key_fields:
            if field in row and pd.notna(row[field]):
                key_values.append(str(row[field]).strip())
            else:
                key_values.append("")  # ç©ºå€¼å¤„ç†
        
        # ç”Ÿæˆhashï¼ˆä¸å«æ–‡ä»¶åï¼‰
        key_str = "|".join(key_values)
        return hashlib.md5(key_str.encode('utf-8')).hexdigest()
    except Exception as e:
        logging.warning(f"ç”Ÿæˆè®°å½•hashå¤±è´¥: {e}")
        return hashlib.md5(str(row.to_dict()).encode('utf-8')).hexdigest()


def load_etl_state(state_file: str) -> Dict[str, Any]:
    """
    åŠ è½½ETLçŠ¶æ€æ–‡ä»¶
    """
    if os.path.exists(state_file):
        try:
            with open(state_file, 'r', encoding='utf-8') as f:
                state = json.load(f)
            
            # ç¡®ä¿processed_hashesæ˜¯setç±»å‹
            if "processed_hashes" in state and isinstance(state["processed_hashes"], list):
                state["processed_hashes"] = set(state["processed_hashes"])
            
            logging.info(f"å·²åŠ è½½ETLçŠ¶æ€: {len(state.get('processed_hashes', []))} æ¡å·²å¤„ç†è®°å½•")
            return state
        except Exception as e:
            logging.warning(f"åŠ è½½çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨ç©ºçŠ¶æ€")
    
    # è¿”å›é»˜è®¤çŠ¶æ€
    return {
        "processed_hashes": set(),
        "processed_files": {},  # æ ¼å¼: {æ–‡ä»¶è·¯å¾„: {mtime: ä¿®æ”¹æ—¶é—´, hash: æ–‡ä»¶hash}}
        "last_update": datetime.now().isoformat(),
        "total_records": 0
    }


def save_etl_state(state: Dict[str, Any], state_file: str) -> None:
    """
    ä¿å­˜ETLçŠ¶æ€æ–‡ä»¶
    """
    try:
        # åˆ›å»ºçŠ¶æ€ç›®å½•
        os.makedirs(os.path.dirname(state_file), exist_ok=True)
        
        # å¤åˆ¶çŠ¶æ€ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        state_copy = state.copy()
        
        # è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
        if "processed_hashes" in state_copy and isinstance(state_copy["processed_hashes"], set):
            state_copy["processed_hashes"] = list(state_copy["processed_hashes"])
        
        # æ›´æ–°æ—¶é—´æˆ³
        state_copy["last_update"] = datetime.now().isoformat()
        
        # ä¿å­˜çŠ¶æ€
        with open(state_file, 'w', encoding='utf-8') as f:
            json.dump(state_copy, f, ensure_ascii=False, indent=2)
        
        logging.info(f"ETLçŠ¶æ€å·²ä¿å­˜: {state_file}")
    except Exception as e:
        logging.error(f"ä¿å­˜çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")


def filter_incremental_data(df: pd.DataFrame, file_path: str, cfg: Dict[str, Any], state_file: str) -> pd.DataFrame:
    """
    å¢é‡è¿‡æ»¤ï¼šåªè¿”å›æ–°æ•°æ®ï¼ˆæœªå¤„ç†è¿‡çš„è®°å½•ï¼‰
    åŸºäºä¸šåŠ¡å­—æ®µè¿›è¡ŒhashåŒ¹é…ï¼ˆä¸å«æ–‡ä»¶åï¼‰
    """
    incr_cfg = cfg.get("incremental", {})
    
    if not incr_cfg.get("enabled", False):
        logging.info("å¢é‡å¤„ç†æœªå¯ç”¨ï¼Œè¿”å›å…¨éƒ¨æ•°æ®")
        return df
    
    # åŠ è½½å¤„ç†çŠ¶æ€
    state = load_etl_state(state_file)
    processed_hashes = state.get("processed_hashes", set())
    
    # æ£€æŸ¥å…³é”®å­—æ®µ
    key_fields = incr_cfg.get("key_fields", [])
    missing_fields = [field for field in key_fields if field not in df.columns]
    
    if missing_fields:
        logging.warning(f"å¢é‡å¤„ç†æ‰€éœ€å­—æ®µä¸å­˜åœ¨: {missing_fields}ï¼Œè¿”å›å…¨éƒ¨æ•°æ®")
        return df
    
    # ç”Ÿæˆæ¯æ¡è®°å½•çš„hashå¹¶ç­›é€‰æ–°æ•°æ®ï¼ˆä¸å«æ–‡ä»¶åï¼‰
    df['_record_hash'] = df.apply(lambda row: generate_record_hash(row, key_fields), axis=1)
    
    # ç­›é€‰æ–°æ•°æ®ï¼ˆhashä¸åœ¨å·²å¤„ç†é›†åˆä¸­ï¼‰
    new_data = df[~df['_record_hash'].isin(processed_hashes)].copy()
    new_data = new_data.drop(columns=['_record_hash'])
    
    logging.info(f"å¢é‡è¿‡æ»¤: {len(df)} â†’ {len(new_data)} æ¡æ–°è®°å½•")
    
    return new_data


def update_etl_state(df: pd.DataFrame, file_path: str, cfg: Dict[str, Any], state_file: str) -> None:
    """
    æ›´æ–°ETLçŠ¶æ€ï¼šè®°å½•å·²å¤„ç†çš„è®°å½•hashå’Œæ–‡ä»¶ä¿¡æ¯
    """
    incr_cfg = cfg.get("incremental", {})
    
    if not incr_cfg.get("enabled", False):
        logging.info("å¢é‡å¤„ç†æœªå¯ç”¨ï¼Œè·³è¿‡çŠ¶æ€æ›´æ–°")
        return
    
    # åŠ è½½å½“å‰çŠ¶æ€
    state = load_etl_state(state_file)
    
    # æ£€æŸ¥å…³é”®å­—æ®µ
    key_fields = incr_cfg.get("key_fields", [])
    missing_fields = [field for field in key_fields if field not in df.columns]
    
    if missing_fields:
        logging.warning(f"å¢é‡å¤„ç†æ‰€éœ€å­—æ®µä¸å­˜åœ¨: {missing_fields}ï¼Œè·³è¿‡çŠ¶æ€æ›´æ–°")
        return
    
    # ç”Ÿæˆæ‰€æœ‰è®°å½•çš„hashï¼ˆä¸å«æ–‡ä»¶åï¼‰
    df['_record_hash'] = df.apply(lambda row: generate_record_hash(row, key_fields), axis=1)
    new_hashes = set(df['_record_hash'].unique())
    
    # æ›´æ–°å·²å¤„ç†çš„hashé›†åˆ
    processed_hashes = state.get("processed_hashes", set())
    processed_hashes.update(new_hashes)
    state["processed_hashes"] = processed_hashes
    
    # æ›´æ–°æ–‡ä»¶ä¿¡æ¯ï¼ˆä»…åœ¨æ–‡ä»¶è·¯å¾„æœ‰æ•ˆæ—¶ï¼‰
    file_name = os.path.basename(file_path) if file_path and os.path.exists(file_path) else "processed_data"
    if "processed_files" not in state:
        state["processed_files"] = {}
    
    file_info = {
        "record_count": len(df),
        "processed_time": datetime.now().isoformat(),
        "new_record_count": len(new_hashes)
    }
    
    # ä»…åœ¨æ–‡ä»¶è·¯å¾„æœ‰æ•ˆæ—¶æ·»åŠ æ–‡ä»¶å…ƒæ•°æ®
    if file_path and os.path.exists(file_path):
        file_info.update({
            "mtime": os.path.getmtime(file_path),
            "hash": hashlib.md5(str(len(df)).encode('utf-8')).hexdigest()
        })
    
    state["processed_files"][file_name] = file_info
    
    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
    state["total_records"] = len(processed_hashes)
    
    # ä¿å­˜çŠ¶æ€
    save_etl_state(state, state_file)
    
    logging.info(f"çŠ¶æ€æ›´æ–°å®Œæˆï¼šæ–‡ä»¶ {file_name}ï¼Œæ–°è®°å½• {len(new_hashes)} æ¡ï¼Œæ€»è®°å½•æ•° {len(processed_hashes)} æ¡")


def apply_field_mapping(df: pd.DataFrame, cfg: Dict[str, Any]) -> pd.DataFrame:
    """
    åº”ç”¨å­—æ®µæ˜ å°„
    """
    field_mapping = cfg.get("field_mapping", {})
    
    if not field_mapping:
        return df
    
    result = df.copy()
    renamed_columns = {}
    
    for old_name, new_name in field_mapping.items():
        if old_name in result.columns:
            renamed_columns[old_name] = new_name
    
    if renamed_columns:
        result = result.rename(columns=renamed_columns)
        logging.info(f"å­—æ®µæ˜ å°„å®Œæˆ: {len(renamed_columns)} ä¸ªå­—æ®µé‡å‘½å")
    
    return result


def apply_type_conversion(df: pd.DataFrame, cfg: Dict[str, Any]) -> pd.DataFrame:
    """
    åº”ç”¨æ•°æ®ç±»å‹è½¬æ¢
    """
    type_config = cfg.get("types", {})
    
    if not type_config:
        return df
    
    result = df.copy()
    converted_count = 0
    
    for field, target_type in type_config.items():
        if field in result.columns:
            try:
                if target_type == "datetime":
                    result[field] = pd.to_datetime(result[field])
                elif target_type == "int":
                    result[field] = pd.to_numeric(result[field], errors='coerce').fillna(0).astype('int64')
                elif target_type == "float":
                    result[field] = pd.to_numeric(result[field], errors='coerce')
                elif target_type == "string":
                    result[field] = result[field].astype(str)
                else:
                    result[field] = result[field].astype(target_type)
                
                converted_count += 1
            except Exception as e:
                logging.warning(f"å­—æ®µ {field} ç±»å‹è½¬æ¢å¤±è´¥ ({target_type}): {e}")
    
    if converted_count > 0:
        logging.info(f"æ•°æ®ç±»å‹è½¬æ¢å®Œæˆ: {converted_count} ä¸ªå­—æ®µ")
    
    return result


def read_product_inspection_data(config: dict, incr_processor: IncrementalProcessor = None) -> pd.DataFrame:
    """
    è¯»å–äº§å“æ£€éªŒè®°å½•æ•°æ®
    æ”¯æŒå¤šæ–‡ä»¶è¯»å–å’Œåˆå¹¶ï¼Œæ”¯æŒæ–‡ä»¶çº§å¢é‡å¤„ç†
    
    Args:
        config: é…ç½®å­—å…¸
        incr_processor: å¢é‡å¤„ç†å™¨ï¼ˆå¯é€‰ï¼Œç”¨äºæ–‡ä»¶çº§å»é‡ï¼‰
    """
    source_cfg = config.get("source", {})
    data_path = source_cfg.get("product_inspection_path", "")
    
    if not data_path:
        raise ValueError("æœªé…ç½®äº§å“æ£€éªŒè®°å½•æ•°æ®è·¯å¾„")
    
    logging.info(f"è¯»å–äº§å“æ£€éªŒè®°å½•æ•°æ®: {data_path}")
    
    # å¤„ç†é€šé…ç¬¦è·¯å¾„
    if "*" in data_path or "?" in data_path:
        data_files = glob.glob(data_path)
        if not data_files:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°åŒ¹é…çš„æ•°æ®æ–‡ä»¶: {data_path}")
        
        # æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæœ€æ–°æ–‡ä»¶ä¼˜å…ˆï¼‰
        data_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
        logging.info(f"æ‰¾åˆ° {len(data_files)} ä¸ªäº§å“æ£€éªŒè®°å½•æ•°æ®æ–‡ä»¶ï¼ŒæŒ‰ä¿®æ”¹æ—¶é—´æ’åº")
        
        # æµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶æ–‡ä»¶æ•°é‡
        test_cfg = config.get("test", {})
        test_enabled = test_cfg.get("enabled", False)
        max_files = test_cfg.get("max_files", 20) if test_enabled else len(data_files)
        
        if test_enabled and len(data_files) > max_files:
            data_files = data_files[:max_files]
            logging.info(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šä»…è¯»å–å‰ {max_files} ä¸ªæœ€æ–°æ–‡ä»¶")
        
        # ç¬¬1å±‚ï¼šæ–‡ä»¶çº§å»é‡ï¼ˆå¦‚æœæä¾›äº†å¢é‡å¤„ç†å™¨ï¼‰
        if incr_processor:
            original_count = len(data_files)
            data_files = incr_processor.filter_changed_files(data_files)
            if not data_files:
                logging.info("æ‰€æœ‰æ–‡ä»¶æœªå˜åŒ–ï¼Œæ— éœ€è¯»å–")
                return pd.DataFrame()
        
        # è¯»å–æ‰€æœ‰æ–‡ä»¶å¹¶åˆå¹¶
        dfs = []
        for i, file_path in enumerate(data_files, start=1):
            try:
                mod_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                logging.info(f"è¯»å–æ–‡ä»¶ {i}/{len(data_files)}: {os.path.basename(file_path)} (ä¿®æ”¹æ—¶é—´: {mod_time})")
                df = read_sharepoint_excel(file_path)
                if not df.empty:
                    # æ·»åŠ æ–‡ä»¶ä¿¡æ¯å­—æ®µï¼ˆä½¿ç”¨ä¸­æ–‡å­—æ®µåï¼Œåç»­æ˜ å°„ï¼‰
                    df['source_file'] = os.path.basename(file_path)
                    df['file_mod_time'] = datetime.fromtimestamp(os.path.getmtime(file_path))
                    
                    # ä»æ–‡ä»¶åæå–æŠ¥è¡¨æ—¥æœŸï¼ˆä½¿ç”¨ä¸­æ–‡å­—æ®µåï¼Œåç»­æ˜ å°„ï¼‰
                    filename = os.path.basename(file_path)
                    report_date = extract_report_date_from_filename(filename)
                    df['æŠ¥è¡¨æ—¥æœŸ'] = report_date
                    
                    dfs.append(df)
                    
                    # æ ‡è®°æ–‡ä»¶å·²å¤„ç†
                    if incr_processor:
                        incr_processor.mark_file_processed(file_path)
            except Exception as e:
                logging.warning(f"è¯»å–æ–‡ä»¶å¤±è´¥ {i}/{len(data_files)}: {file_path}: {e}")
        
        if not dfs:
            raise ValueError("æ‰€æœ‰äº§å“æ£€éªŒè®°å½•æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥")
        
        combined_df = pd.concat(dfs, ignore_index=True)
        logging.info(f"åˆå¹¶åäº§å“æ£€éªŒè®°å½•æ•°æ®è¡Œæ•°: {len(combined_df)}")
        return combined_df
    
    else:
        # å•æ–‡ä»¶å¤„ç†
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"äº§å“æ£€éªŒè®°å½•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        
        df = read_sharepoint_excel(data_path)
        if not df.empty:
            # ä»æ–‡ä»¶åæå–æŠ¥è¡¨æ—¥æœŸ
            filename = os.path.basename(data_path)
            report_date = extract_report_date_from_filename(filename)
            df['æŠ¥è¡¨æ—¥æœŸ'] = report_date
            
        return df


def validate_product_inspection_data(df: pd.DataFrame, config: dict) -> bool:
    """
    éªŒè¯äº§å“æ£€éªŒè®°å½•æ•°æ®å®Œæ•´æ€§
    """
    logging.info("å¼€å§‹éªŒè¯äº§å“æ£€éªŒè®°å½•æ•°æ®å®Œæ•´æ€§...")
    
    # æ£€æŸ¥å¿…è¦å­—æ®µ
    required_fields = config.get("processing", {}).get("required_fields", 
        ["æ‰¹æ¬¡å·", "äº§å“åºå·", "æ£€éªŒç»“æœ", "ç­ç»„"])
    # Note: åˆæ ¼æ•° and ä¸åˆæ ¼æ•° will be derived during cleaning
    
    missing_fields = [field for field in required_fields if field not in df.columns]
    if missing_fields:
        logging.error(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}")
        return False
    
    # æ£€æŸ¥æ•°æ®è¡Œæ•°
    if df.empty:
        logging.error("äº§å“æ£€éªŒè®°å½•æ•°æ®ä¸ºç©º")
        return False
    
    # æ£€æŸ¥å…³é”®å­—æ®µç¼ºå¤±ç‡
    for field in required_fields:
        missing_count = df[field].isnull().sum()
        missing_rate = missing_count / len(df) * 100
        if missing_rate > 5:  # ç¼ºå¤±ç‡è¶…è¿‡5%æŠ¥è­¦
            logging.warning(f"å­—æ®µ {field} ç¼ºå¤±ç‡è¾ƒé«˜: {missing_rate:.1f}% ({missing_count}/{len(df)})")
    
    logging.info(f"äº§å“æ£€éªŒè®°å½•æ•°æ®éªŒè¯é€šè¿‡: {len(df)} è¡Œè®°å½•")
    return True


def clean_product_inspection_data(df: pd.DataFrame, config: dict) -> pd.DataFrame:
    """
    æ¸…æ´—äº§å“æ£€éªŒè®°å½•æ•°æ®
    """
    logging.info("å¼€å§‹æ¸…æ´—äº§å“æ£€éªŒè®°å½•æ•°æ®...")
    
    result = df.copy()
    original_count = len(result)
    
    # 1. æ ‡å‡†åŒ–å­—æ®µåï¼ˆå»é™¤ç©ºæ ¼ï¼‰
    result.columns = result.columns.str.strip()
    
    # 2. æ ‡å‡†åŒ–æ‰¹æ¬¡å·ï¼šç§»é™¤IPQCKå‰ç¼€
    if 'æ‰¹æ¬¡å·' in result.columns:
        result['æ‰¹æ¬¡å·'] = result['æ‰¹æ¬¡å·'].astype(str).str.replace(r'^IPQCK', '', regex=True)
        logging.info("å·²æ ‡å‡†åŒ–æ‰¹æ¬¡å·ï¼šç§»é™¤IPQCKå‰ç¼€")
    
    # 3. å¤„ç†æœºåºŠè®¾å¤‡ç¼ºå¤±å€¼
    if 'æœºåºŠè®¾å¤‡' in result.columns:
        missing_equipment = result['æœºåºŠè®¾å¤‡'].isnull()
        if missing_equipment.any():
            result['æœºåºŠè®¾å¤‡'] = result['æœºåºŠè®¾å¤‡'].astype('object')
            result.loc[missing_equipment, 'æœºåºŠè®¾å¤‡'] = result.loc[missing_equipment, 'å·¥åºç¼–å·'].astype(str)
            logging.info(f"å·²ç”¨å·¥åºç¼–å·å¡«å…… {missing_equipment.sum()} ä¸ªç¼ºå¤±çš„æœºåºŠè®¾å¤‡")
    
    # 4. å¤„ç†äº§å“åºå·ç¼ºå¤±å€¼
    if 'äº§å“åºå·' in result.columns:
        missing_serial = result['äº§å“åºå·'].isnull()
        if missing_serial.any():
            result.loc[missing_serial, 'äº§å“åºå·'] = result.loc[missing_serial].groupby('æ‰¹æ¬¡å·').cumcount() + 1
            logging.info(f"å·²å¡«å…… {missing_serial.sum()} ä¸ªç¼ºå¤±çš„äº§å“åºå·")
    
    # 5. æ ‡å‡†åŒ–æ£€éªŒç»“æœå¹¶æå–åˆæ ¼æ•°/ä¸åˆæ ¼æ•°
    if 'æ£€éªŒç»“æœ' in result.columns:
        result['æ£€éªŒç»“æœ'] = result['æ£€éªŒç»“æœ'].str.strip().str.upper()
        
        # åˆå§‹åŒ–åˆæ ¼æ•°å’Œä¸åˆæ ¼æ•°åˆ—
        result['åˆæ ¼æ•°'] = 0
        result['ä¸åˆæ ¼æ•°'] = 0
        
        # æå–åˆæ ¼æ•°å’Œä¸åˆæ ¼æ•°
        import re
        
        # å¤„ç†åŒ…å«å…·ä½“æ•°å€¼çš„æ ¼å¼ï¼šåˆæ ¼æ•°ï¼š100 / ä¸åˆæ ¼æ•°ï¼š0
        count_pattern = r'åˆæ ¼æ•°[ï¼š:]\s*(\d+)\s*/\s*ä¸åˆæ ¼æ•°[ï¼š:]\s*(\d+)'
        count_matches = result['æ£€éªŒç»“æœ'].str.extract(count_pattern)
        
        # å¯¹äºåŒ¹é…åˆ°çš„è®°å½•ï¼Œè®¾ç½®åˆæ ¼æ•°å’Œä¸åˆæ ¼æ•°
        mask = count_matches[0].notna()
        if mask.any():
            result.loc[mask, 'åˆæ ¼æ•°'] = pd.to_numeric(count_matches.loc[mask, 0])
            result.loc[mask, 'ä¸åˆæ ¼æ•°'] = pd.to_numeric(count_matches.loc[mask, 1])
        
        # å¯¹äºç®€å•çš„"åˆæ ¼"æˆ–"ä¸åˆæ ¼"è®°å½•
        simple_mask = result['æ£€éªŒç»“æœ'].isin(['åˆæ ¼', 'PASS'])
        result.loc[simple_mask & ~mask, 'åˆæ ¼æ•°'] = 1
        result.loc[simple_mask & ~mask, 'ä¸åˆæ ¼æ•°'] = 0
        
        fail_mask = result['æ£€éªŒç»“æœ'].isin(['ä¸åˆæ ¼', 'FAIL'])
        result.loc[fail_mask & ~mask, 'åˆæ ¼æ•°'] = 0
        result.loc[fail_mask & ~mask, 'ä¸åˆæ ¼æ•°'] = 1
        
        # æ ‡å‡†åŒ–æ£€éªŒç»“æœä¸ºç®€å•å€¼
        result.loc[result['æ£€éªŒç»“æœ'].str.contains('åˆæ ¼', na=False) | result['æ£€éªŒç»“æœ'].str.contains('PASS', na=False), 'æ£€éªŒç»“æœ'] = 'åˆæ ¼'
        result.loc[result['æ£€éªŒç»“æœ'].str.contains('ä¸åˆæ ¼', na=False) | result['æ£€éªŒç»“æœ'].str.contains('FAIL', na=False), 'æ£€éªŒç»“æœ'] = 'ä¸åˆæ ¼'
        
        logging.info(f"å·²æå–åˆæ ¼æ•°/ä¸åˆæ ¼æ•°ä¿¡æ¯")
    
    # 6. ä¸šåŠ¡é”®å»é‡ï¼ˆä¿ç•™æœ€æ–°æ•°æ®ï¼‰
    before_dedup = len(result)
    
    # æ£€æŸ¥ä¸šåŠ¡é”®æ˜¯å¦å­˜åœ¨
    business_keys = ['æ‰¹æ¬¡å·', 'äº§å“åºå·', 'å·¥åºç¼–å·', 'å·¥åºåç§°']
    missing_keys = [key for key in business_keys if key not in result.columns]
    
    if missing_keys:
        logging.warning(f"ç¼ºå°‘ä¸šåŠ¡é”®å­—æ®µ {missing_keys}ï¼Œä½¿ç”¨æ‰¹æ¬¡+å·¥åº+ç­ç»„å»é‡")
        backup_keys = ['æ‰¹æ¬¡å·', 'å·¥åºç¼–å·', 'å·¥åºåç§°', 'ç­ç»„']
        backup_missing = [key for key in backup_keys if key not in result.columns]
        if backup_missing:
            logging.warning(f"ç¼ºå°‘å¤‡é€‰ä¸šåŠ¡é”®å­—æ®µ {backup_missing}ï¼Œä½¿ç”¨å…¨è¡Œå»é‡")
            result = result.drop_duplicates()
        else:
            result = result.drop_duplicates(subset=backup_keys, keep='first')
            logging.info(f"å¤‡é€‰ä¸šåŠ¡é”®å»é‡: åŸºäºæ‰¹æ¬¡+å·¥åº+ç­ç»„ç§»é™¤é‡å¤è®°å½•")
    else:
        before_business_dedup = len(result)
        result = result.drop_duplicates(subset=business_keys, keep='first')
        
        business_dedup_count = before_business_dedup - len(result)
        logging.info(f"ä¸šåŠ¡é”®å»é‡: åŸºäºæ‰¹æ¬¡å·+äº§å“åºå·+å·¥åºç¼–å·+å·¥åºåç§°ç§»é™¤ {business_dedup_count} æ¡é‡å¤è®°å½•")
        
        # å¦‚æœä»æœ‰å®Œå…¨é‡å¤çš„è¡Œï¼Œå†è¿›è¡Œä¸€æ¬¡å…¨è¡Œå»é‡
        final_before = len(result)
        result = result.drop_duplicates()
        final_dedup_count = final_before - len(result)
        
        if final_dedup_count > 0:
            logging.info(f"å…¨è¡Œå»é‡: ç§»é™¤ {final_dedup_count} æ¡å®Œå…¨é‡å¤è®°å½•")
    
    removed_duplicates = before_dedup - len(result)
    logging.info(f"æ€»å»é‡ç»Ÿè®¡: ç§»é™¤ {removed_duplicates} æ¡é‡å¤è®°å½•")
    
    logging.info(f"äº§å“æ£€éªŒè®°å½•æ•°æ®æ¸…æ´—å®Œæˆ: {original_count} â†’ {len(result)} è¡Œ")
    return result


def aggregate_employee_efficiency_data(df: pd.DataFrame, config: dict) -> pd.DataFrame:
    """
    æŒ‰å‘˜å·¥+æ—¥æœŸ+æ‰¹æ¬¡+å·¥åºèšåˆï¼Œç”¨äºè®¡ç®—å‘˜å·¥æ•ˆç‡
    èšåˆç»´åº¦ï¼šå‘˜å·¥ã€æ—¥æœŸã€æ‰¹æ¬¡å·ã€å·¥åºç¼–å·
    æŒ‡æ ‡ï¼šåˆæ ¼æ•°ã€ä¸åˆæ ¼æ•°ã€æœ€å¤§äº§å“åºå·ï¼ˆå®Œæˆæ•°é‡ï¼‰
    """
    if df.empty:
        return df
    
    result = df.copy()
    logging.info("å¼€å§‹æŒ‰å‘˜å·¥è®¡ç®—æ•ˆç‡æŒ‡æ ‡...")
    
    # æ£€æŸ¥å¿…è¦çš„å­—æ®µ
    required_fields = ['Employee', 'ReportDate', 'BatchNumber', 'Operation', 'PassQuantity', 'FailQuantity', 'SerialNumber']
    missing_fields = [field for field in required_fields if field not in result.columns]
    if missing_fields:
        logging.error(f"å‘˜å·¥èšåˆæ‰€éœ€å­—æ®µç¼ºå¤±: {missing_fields}")
        return result
    
    # æå–æ—¥æœŸç»´åº¦ï¼ˆä»ReportDateæå–å¹´æœˆæ—¥ï¼‰
    result['ReportDate_Date'] = result['ReportDate'].dt.date
    result['ReportDate_Year'] = result['ReportDate'].dt.year
    result['ReportDate_Month'] = result['ReportDate'].dt.month
    result['ReportDate_Day'] = result['ReportDate'].dt.day
    result['ReportDate_Week'] = result['ReportDate'].dt.isocalendar().week
    
    # æŒ‰å‘˜å·¥+æ—¥æœŸ+æ‰¹æ¬¡+å·¥åºèšåˆ
    employee_agg = result.groupby([
        'Employee', 
        'ReportDate_Date', 
        'ReportDate_Year', 
        'ReportDate_Month', 
        'ReportDate_Day', 
        'ReportDate_Week',
        'BatchNumber', 
        'Operation', 
        'OperationDescription',
        'Team'
    ]).agg(
        PassQuantity=('PassQuantity', 'sum'),
        FailQuantity=('FailQuantity', 'sum'),
        MaxSerialNumber=('SerialNumber', 'max'),
        CompletedCount=('SerialNumber', 'max'),  # æœ€å¤§äº§å“åºå·ä½œä¸ºå®Œæˆæ•°é‡
        OperationCount=('SerialNumber', 'count'),  # æ“ä½œæ¬¡æ•°
        FileModTime=('FileModTime', 'max'),
        SourceFile=('SourceFile', lambda x: ', '.join(set(x.dropna().astype(str).str.strip())))
    ).reset_index()
    
    logging.info(f"å‘˜å·¥çº§èšåˆå®Œæˆ: {len(result)} â†’ {len(employee_agg)} æ¡è®°å½•")
    
    # è®¡ç®—åˆæ ¼ç‡
    employee_agg['PassRate'] = employee_agg.apply(
        lambda row: row['PassQuantity'] / (row['PassQuantity'] + row['FailQuantity']) 
        if (row['PassQuantity'] + row['FailQuantity']) > 0 else 0, 
        axis=1
    )
    
    # è®¡ç®—æ•ˆç‡æŒ‡æ ‡
    employee_agg['Efficiency_Score'] = employee_agg['PassRate']  # å¯ä»¥æ‰©å±•ä¸ºæ›´å¤æ‚çš„æ•ˆç‡è®¡ç®—
    
    # æ·»åŠ æ—¶é—´æˆ³
    employee_agg['AggregatedTime'] = pd.Timestamp.now()
    
    # é‡å‘½åå­—æ®µä»¥ä¾¿PowerBIä½¿ç”¨
    employee_agg = employee_agg.rename(columns={
        'ReportDate_Date': 'Date',
        'ReportDate_Year': 'Year',
        'ReportDate_Month': 'Month', 
        'ReportDate_Day': 'Day',
        'ReportDate_Week': 'Week',
        'MaxSerialNumber': 'MaxSerial',
        'CompletedCount': 'CompletedQuantity',
        'OperationCount': 'OperationCount'
    })
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_employees = employee_agg['Employee'].nunique()
    total_days = employee_agg['Date'].nunique()
    avg_pass_rate = employee_agg['PassRate'].mean()
    total_completed = employee_agg['CompletedQuantity'].sum()
    
    logging.info(f"å‘˜å·¥æ•ˆç‡ç»Ÿè®¡: {total_employees} ä¸ªå‘˜å·¥, {total_days} å¤©, å¹³å‡åˆæ ¼ç‡ {avg_pass_rate:.4f}, æ€»å®Œæˆæ•° {total_completed}")
    
    return employee_agg


def aggregate_product_inspection_data_batch_level(df: pd.DataFrame, config: dict) -> pd.DataFrame:
    """
    æŒ‰æ‰¹æ¬¡è®¡ç®—çœŸå®åˆæ ¼ç‡ï¼šä»¥æ‰¹æ¬¡ä¸ºå•ä½ï¼Œä½¿ç”¨æœ€ç»ˆå·¥åºåˆæ ¼æ•°/ï¼ˆæœ€ç»ˆå·¥åºåˆæ ¼æ•°+æ‰€æœ‰å·¥åºä¸åˆæ ¼æ•°æ€»å’Œï¼‰
    """
    if df.empty:
        return df
    
    result = df.copy()
    logging.info("å¼€å§‹æŒ‰æ‰¹æ¬¡è®¡ç®—çœŸå®åˆæ ¼ç‡...")
    
    # æ£€æŸ¥å¿…è¦çš„å­—æ®µ
    required_fields = ['BatchNumber', 'Operation', 'OperationDescription', 'Team', 'Machine']
    missing_fields = [field for field in required_fields if field not in result.columns]
    if missing_fields:
        logging.error(f"èšåˆæ‰€éœ€å­—æ®µç¼ºå¤±: {missing_fields}")
        return result
    
    # æŒ‰æ‰¹æ¬¡+äº§å“å·+å·¥åºç¼–å·+ç­ç»„+æœºåºŠè®¾å¤‡èšåˆï¼Œä¿ç•™äº§å“åºå·ä¿¡æ¯
    operation_agg = result.groupby(['BatchNumber', 'Operation', 'OperationDescription', 'Team', 'Machine']).agg(
        PassQuantity=('PassQuantity', 'sum'),
        FailQuantity=('FailQuantity', 'sum'),
        SerialNumber_count=('SerialNumber', 'count'),
        SerialNumber=('SerialNumber', 'first'),  # ä¿ç•™ç¬¬ä¸€ä¸ªäº§å“åºå·
        FileModTime=('FileModTime', 'max'),
        SourceFile=('SourceFile', lambda x: ', '.join(set(x.dropna().astype(str).str.strip())))
    ).reset_index()
    
    logging.info(f"å·¥åºçº§èšåˆå®Œæˆ: {len(result)} â†’ {len(operation_agg)} æ¡è®°å½•")
    
    # è¿‡æ»¤æ‰æ— æ•ˆå·¥åºç¼–å·
    if 'Operation' in operation_agg.columns:
        valid_operations = operation_agg['Operation'].notna()
        operation_agg = operation_agg[valid_operations]
        logging.info(f"è¿‡æ»¤æ— æ•ˆå·¥åºç¼–å·: {len(operation_agg)} æ¡è®°å½•")
    
    # æŒ‰æ‰¹æ¬¡è®¡ç®—æœ€ç»ˆåˆæ ¼ç‡
    def calculate_batch_pass_rate(group):
        # æ‰¾åˆ°æœ€ç»ˆå·¥åºï¼ˆå·¥åºç¼–å·æœ€å¤§ï¼‰
        final_operation = group.loc[group['Operation'].astype(str).str.zfill(4).idxmax()]
        
        # æœ€ç»ˆå·¥åºåˆæ ¼æ•°
        final_pass = final_operation['PassQuantity']
        
        # æ‰€æœ‰å·¥åºä¸åˆæ ¼æ•°æ€»å’Œ
        total_fail = group['FailQuantity'].sum()
        
        # è®¡ç®—åˆæ ¼ç‡
        total_count = final_pass + total_fail
        if total_count > 0:
            pass_rate = final_pass / total_count
        else:
            pass_rate = 0
        
        # è¿”å›æœ€ç»ˆå·¥åºçš„å®Œæ•´ä¿¡æ¯ï¼ŒåŠ ä¸Šåˆæ ¼ç‡
        result_row = final_operation.copy()
        result_row['PassRate'] = pass_rate
        result_row['TotalInspectionCount'] = total_count
        
        return result_row
    
    # æŒ‰æ‰¹æ¬¡åˆ†ç»„è®¡ç®—
    batch_results = []
    for batch_name, batch_group in operation_agg.groupby('BatchNumber'):
        try:
            batch_result = calculate_batch_pass_rate(batch_group)
            batch_results.append(batch_result)
        except Exception as e:
            logging.warning(f"æ‰¹æ¬¡ {batch_name} è®¡ç®—å¤±è´¥: {e}")
            continue
    
    if batch_results:
        batch_df = pd.DataFrame(batch_results)
        logging.info(f"æ‰¹æ¬¡çº§åˆæ ¼ç‡è®¡ç®—å®Œæˆ: {len(operation_agg)} â†’ {len(batch_df)} æ¡è®°å½•")
        
        # è®¡ç®—å¹³å‡åˆæ ¼ç‡
        avg_pass_rate = batch_df['PassRate'].mean()
        logging.info(f"å¹³å‡æ‰¹æ¬¡åˆæ ¼ç‡: {avg_pass_rate:.4f} (ç™¾åˆ†æ¯”æ ¼å¼)")
        
        return batch_df
    else:
        logging.error("æ‰¹æ¬¡çº§èšåˆå¤±è´¥ï¼Œè¿”å›ç©ºæ•°æ®")
        return pd.DataFrame()


def save_to_parquet(df: pd.DataFrame, file_path: str) -> bool:
    """
    ä¿å­˜æ•°æ®åˆ°Parquetæ–‡ä»¶
    """
    try:
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        if pq is None:
            logging.error("æœªå®‰è£…pyarrowï¼Œæ— æ³•ä¿å­˜Parquetæ–‡ä»¶")
            return False
        
        # è½¬æ¢ä¸ºpyarrowè¡¨
        table = pa.Table.from_pandas(df)
        
        # ä¿å­˜åˆ°parquet
        pq.write_table(table, file_path)
        
        logging.info(f"å·²ä¿å­˜Parquetæ–‡ä»¶: {file_path}, è¡Œæ•°: {len(df)}")
        return True
        
    except Exception as e:
        logging.error(f"ä¿å­˜Parquetæ–‡ä»¶å¤±è´¥: {e}")
        return False


def save_to_excel(df: pd.DataFrame, file_path: str) -> bool:
    """
    ä¿å­˜æ•°æ®åˆ°Excelæ–‡ä»¶ï¼ˆç”¨äºéªŒè¯ï¼‰
    """
    try:
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        df.to_excel(file_path, index=False)
        
        logging.info(f"å·²ä¿å­˜ExceléªŒè¯æ–‡ä»¶: {file_path}, è¡Œæ•°: {len(df)}")
        return True
        
    except Exception as e:
        logging.error(f"ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("SFCäº§å“æ£€éªŒè®°å½•æ•°æ®æ¸…æ´—ETL")
    print("=" * 60)
    
    # åŠ è½½é…ç½®
    # å›ºå®šä½¿ç”¨å¢é‡åˆ·æ–°æ¨¡å¼ï¼ˆç”±BATè„šæœ¬ç»Ÿä¸€æ§åˆ¶ï¼‰
    # ç§»é™¤å‘½ä»¤è¡Œå‚æ•°ï¼Œä¿æŒETLè„šæœ¬çº¯å‡€
    
    config = load_config(CONFIG_PATH)
    setup_logging_with_rotation(config, project_root)
    
    # å›ºå®šä½¿ç”¨å¢é‡åˆ·æ–°
    is_incremental = True
    force_full_refresh = False
    
    logging.info("="*60)
    logging.info("SFCäº§å“æ£€éªŒETLå¯åŠ¨")
    logging.info("åˆ·æ–°æ¨¡å¼: å¢é‡åˆ·æ–°ï¼ˆå›ºå®šï¼‰")
    logging.info("="*60)
    
    try:
        # åˆå§‹åŒ–å¢é‡å¤„ç†å™¨ï¼ˆçŠ¶æ€æ–‡ä»¶ç»Ÿä¸€æ”¾åˆ°é¡¹ç›®ç›®å½•ï¼‰
        state_dir = os.path.join(project_root, "data_pipelines", "sources", "sfc", "state")
        os.makedirs(state_dir, exist_ok=True)
        state_file = os.path.join(state_dir, "etl_sfc_product_inspection_files.json")
        incr_processor = IncrementalProcessor(
            state_file=state_file,
            unique_key_fields=["BatchNumber", "ProductSerial", "Operation", "OperationName"]
        )
        
        # è¯»å–æ•°æ®ï¼ˆå¸¦æ–‡ä»¶çº§å»é‡ï¼‰
        logging.info("å¼€å§‹è¯»å–äº§å“æ£€éªŒè®°å½•æ•°æ®...")
        raw_df = read_product_inspection_data(config, incr_processor)
        
        # å¦‚æœæ²¡æœ‰æ–°æ–‡ä»¶ï¼Œç›´æ¥è¿”å›
        if raw_df.empty:
            logging.info("æ²¡æœ‰æ–°æ–‡ä»¶éœ€è¦å¤„ç†ï¼Œä¿æŒç°æœ‰æ•°æ®ä¸å˜")
            return True
        
        # éªŒè¯æ•°æ®ï¼ˆä½¿ç”¨ä¸­æ–‡å­—æ®µï¼‰
        if not validate_product_inspection_data(raw_df, config):
            logging.error("æ•°æ®éªŒè¯å¤±è´¥")
            return False
        
        # æ¸…æ´—æ•°æ®ï¼ˆä½¿ç”¨ä¸­æ–‡å­—æ®µï¼‰
        cleaned_df = clean_product_inspection_data(raw_df, config)
        
        # åº”ç”¨å­—æ®µæ˜ å°„ï¼ˆä¸­æ–‡åˆ°è‹±æ–‡ï¼‰
        mapped_df = apply_field_mapping(cleaned_df, config)
        
        # åº”ç”¨ç±»å‹è½¬æ¢ï¼ˆä½¿ç”¨è‹±æ–‡å­—æ®µï¼‰
        typed_df = apply_type_conversion(mapped_df, config)
        
        # å¢é‡è¿‡æ»¤
        if is_incremental and not force_full_refresh:
            incr_cfg = config.get("incremental", {})
            if incr_cfg.get("enabled", False):
                filtered_df = filter_incremental_data(typed_df, "", config, STATE_PATH)
            else:
                filtered_df = typed_df
        else:
            filtered_df = typed_df
        
        # ä»…æŒ‰å‘˜å·¥èšåˆæ•°æ®ï¼ˆPowerBIå¯é€šè¿‡DAXå®ç°æ‰¹æ¬¡çº§åˆ†æ
        employee_aggregated_df = aggregate_employee_efficiency_data(filtered_df, config)
        
        # ä¿å­˜ç»“æœåˆ°åŒä»½ä½ç½®
        output_cfg = config.get("output", {})
        base_dir = output_cfg.get("base_dir", "")
        
        # æœ¬åœ°é¡¹ç›®ç›®å½•
        local_publish_dir = os.path.join(project_root, "data_pipelines", "sources", "sfc", "publish")
        os.makedirs(local_publish_dir, exist_ok=True)
        
        # å‘˜å·¥çº§ä¸»è¾“å‡ºæ–‡ä»¶ï¼ˆé‡å‘½åä¸ºäº§å“æ£€éªŒè®°å½•ä¸»æ–‡ä»¶
        parquet_file = os.path.join(base_dir, output_cfg.get("file_name", "SFC_Product_Inspection_latest.parquet"))
        local_parquet_file = os.path.join(local_publish_dir, output_cfg.get("file_name", "SFC_Product_Inspection_latest.parquet"))
        excel_file = os.path.join(base_dir, "excel", output_cfg.get("excel_file", "SFC_Product_Inspection_latest.xlsx"))
        
        # ä¿å­˜å‘˜å·¥çº§æ–‡ä»¶åˆ°åŒä»½ä½ç½®
        if not employee_aggregated_df.empty:
            if WRITE_PARQUET_OUTPUT:
                save_to_dual_locations(employee_aggregated_df, parquet_file, local_parquet_file, config)
                logging.info(f"äº§å“æ£€éªŒè®°å½•æ•°æ®å·²ä¿å­˜åˆ°åŒä»½ä½ç½®")
            
            if save_to_excel(employee_aggregated_df, excel_file):
                logging.info(f"äº§å“æ£€éªŒè®°å½•ExceléªŒè¯æ–‡ä»¶å·²ä¿å­˜: {excel_file}")

            try:
                db = get_default_db_manager()
                db.bulk_insert(employee_aggregated_df, 'sfc_product_inspection_latest', if_exists='replace')
                logging.info(f"å·²å†™å…¥SQLiteè¡¨ sfc_product_inspection_latest: {len(employee_aggregated_df)} è¡Œ")
            except Exception as e:
                logging.warning(f"å†™å…¥SQLiteè¡¨ sfc_product_inspection_latest å¤±è´¥: {e}")
        else:
            logging.info("æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡æ–‡ä»¶è¾“å‡º")
        
        # æµ‹è¯•æ¨¡å¼é¢å¤–ä¿å­˜å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶
        test_cfg = config.get("test", {})
        if test_cfg.get("enabled", False):
            # ä½¿ç”¨æ–°çš„ä¸šåŠ¡åŸŸæµ‹è¯•æ•°æ®ç›®å½•
            test_output_dir = resolver.get_path("test_data_paths", "sfc")
            if not os.path.exists(test_output_dir):
                os.makedirs(test_output_dir, exist_ok=True)
            
            # ç”Ÿæˆæ—¶é—´æˆ³æ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # å‘˜å·¥çº§æµ‹è¯•æ–‡ä»¶
            if not employee_aggregated_df.empty:
                test_file = os.path.join(test_output_dir, f"SFC_Product_Inspection_{timestamp}.parquet")
                if WRITE_PARQUET_OUTPUT:
                    if save_to_parquet(employee_aggregated_df, test_file):
                        logging.info(f"æµ‹è¯•æ•°æ®å·²ä¿å­˜: {test_file}")
        
        # æ›´æ–°å¢é‡çŠ¶æ€
        if is_incremental and not force_full_refresh:
            incr_cfg = config.get("incremental", {})
            if incr_cfg.get("enabled", False):
                update_etl_state(typed_df, "processed_data", config, STATE_PATH)
                logging.info("å¢é‡å¤„ç†ï¼šçŠ¶æ€å·²æ›´æ–°")
        
        # ä¿å­˜æ–‡ä»¶çº§å¢é‡çŠ¶æ€
        incr_processor.save()
        
        logging.info("SFCäº§å“æ£€éªŒè®°å½•æ•°æ®å¤„ç†å®Œæˆ")
        return True
        
    except Exception as e:
        logging.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
