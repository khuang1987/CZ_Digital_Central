"""
SAæŒ‡æ ‡æ•°æ®æ¸…æ´—ç»“æœéªŒè¯è„šæœ¬
ç”¨äºç¡®è®¤ETLæ¸…æ´—ç»“æœçš„æ­£ç¡®æ€§ï¼ŒåŒ…æ‹¬å…³é”®è®¡ç®—ç»“æœçš„æŠ½æ ·æ£€æŸ¥
å‘½åè§„èŒƒ: etl_validate_{åŠŸèƒ½æ¨¡å—}_{æ•°æ®ç±»å‹}.py
"""

import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging
from typing import Dict, Any, List, Tuple

# å¯¼å…¥ETLå·¥å…·å‡½æ•°
from etl_utils import load_config, setup_logging

# åŠ è½½é…ç½®æ–‡ä»¶
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CONFIG_PATH = os.path.join(BASE_DIR, "..", "03_é…ç½®æ–‡ä»¶", "config", "config_validate_sa.yaml")
cfg = load_config(CONFIG_PATH)


def load_latest_data(data_type: str) -> pd.DataFrame:
    """
    åŠ è½½æœ€æ–°çš„å¤„ç†åæ•°æ®
    Args:
        data_type: æ•°æ®ç±»å‹ (sfc, mes, sap_routing)
    Returns:
        å¤„ç†åçš„DataFrame
    """
    base_dir = os.path.dirname(os.path.abspath(__file__))
    publish_dir = cfg.get("data_paths", {}).get("publish_dir", r"C:\Users\huangk14\OneDrive - Medtronic PLC\CZ Production - æ–‡æ¡£\General\POWER BI æ•°æ®æº V2\30-MESå¯¼å‡ºæ•°æ®\publish")
    file_mapping = cfg.get("data_paths", {}).get("file_mapping", {
        'sfc': 'SFC_batch_report_latest.parquet',
        'mes': 'MES_batch_report_latest.parquet',
        'sap_routing': 'SAP_Routing_latest.parquet'
    })
    
    if data_type not in file_mapping:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type}")
    
    file_path = os.path.join(publish_dir, file_mapping[data_type])
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    return pd.read_parquet(file_path)


def validate_sfc_calculations(sfc_df: pd.DataFrame, sample_size: int = None) -> Dict[str, Any]:
    """
    éªŒè¯SFCæ•°æ®çš„å…³é”®è®¡ç®—ç»“æœ
    Args:
        sfc_df: SFCå¤„ç†åçš„æ•°æ®
        sample_size: æŠ½æ ·æ•°é‡
    Returns:
        éªŒè¯ç»“æœå­—å…¸
    """
    results = {'status': 'success', 'details': [], 'errors': []}
    
    # ä»é…ç½®æ–‡ä»¶è·å–å‚æ•°
    if sample_size is None:
        sample_size = cfg.get("validation", {}).get("sample_size", 10)
    time_tolerance = cfg.get("validation", {}).get("time_tolerance", 0.1)
    sfc_config = cfg.get("sfc_validation", {})
    
    try:
        # 1. éªŒè¯æ—¶é—´è®¡ç®— (LT, PT, ST)
        time_fields = sfc_config.get("time_fields", ['TrackOutTime', 'CheckInTime', 'EnterStepTime', 'LT', 'PT', 'ST'])
        if all(col in sfc_df.columns for col in time_fields[:3]):  # åªæ£€æŸ¥å‰3ä¸ªæ—¶é—´å­—æ®µ
            valid_data = sfc_df.dropna(subset=time_fields[:3])
            if len(valid_data) > 0:
                sample_df = valid_data.sample(min(sample_size, len(valid_data)))
                
                for idx, row in sample_df.iterrows():
                    track_out = pd.to_datetime(row['TrackOutTime'])
                    check_in = pd.to_datetime(row['CheckInTime'])
                    enter_step = pd.to_datetime(row['EnterStepTime'])
                    
                    # æ‰‹åŠ¨è®¡ç®—æ—¶é—´å·®
                    manual_lt = (track_out - check_in).total_seconds() / 3600  # å°æ—¶
                    manual_pt = (track_out - enter_step).total_seconds() / 3600  # å°æ—¶
                    manual_st = (check_in - enter_step).total_seconds() / 3600  # å°æ—¶
                    
                    # è·å–ç³»ç»Ÿè®¡ç®—çš„å€¼
                    system_lt = row.get('LT', 0) if pd.notna(row.get('LT', 0)) else 0
                    system_pt = row.get('PT', 0) if pd.notna(row.get('PT', 0)) else 0
                    system_st = row.get('ST', 0) if pd.notna(row.get('ST', 0)) else 0
                    
                    # å¯¹æ¯”éªŒè¯ï¼ˆä½¿ç”¨é…ç½®çš„è¯¯å·®å®¹å¿åº¦ï¼‰
                    lt_diff = abs(manual_lt - system_lt)
                    pt_diff = abs(manual_pt - system_pt)
                    st_diff = abs(manual_st - system_st)
                    
                    if lt_diff > time_tolerance or pt_diff > time_tolerance or st_diff > time_tolerance:
                        results['errors'].append({
                            'type': 'æ—¶é—´è®¡ç®—å·®å¼‚',
                            'index': idx,
                            'BatchNumber': row.get('BatchNumber', ''),
                            'Operation': row.get('Operation', ''),
                            'LTå·®å¼‚': round(lt_diff, 3),
                            'PTå·®å¼‚': round(pt_diff, 3),
                            'STå·®å¼‚': round(st_diff, 3)
                        })
                    else:
                        results['details'].append({
                            'type': 'æ—¶é—´è®¡ç®—éªŒè¯',
                            'index': idx,
                            'BatchNumber': row.get('BatchNumber', ''),
                            'Operation': row.get('Operation', ''),
                            'status': 'é€šè¿‡'
                        })
        
        # 2. éªŒè¯æ ‡å‡†æ—¶é—´åˆå¹¶
        std_time_fields = sfc_config.get("standard_time_fields", ['EH_machine(s)'])
        for field in std_time_fields:
            if field in sfc_df.columns:
                # æ£€æŸ¥æ˜¯å¦æœ‰åˆç†çš„æ ‡å‡†æ—¶é—´å€¼
                valid_eh = sfc_df[sfc_df[field].notna() & (sfc_df[field] > 0)]
                if len(valid_eh) > 0:
                    eh_stats = {
                        'æœ€å°å€¼': round(valid_eh[field].min(), 2),
                        'æœ€å¤§å€¼': round(valid_eh[field].max(), 2),
                        'å¹³å‡å€¼': round(valid_eh[field].mean(), 2),
                        'æœ‰æ•ˆè®°å½•æ•°': len(valid_eh),
                        'åŒ¹é…ç‡': f"{len(valid_eh)/len(sfc_df)*100:.1f}%"
                    }
                    results['details'].append({
                        'type': 'æ ‡å‡†æ—¶é—´ç»Ÿè®¡',
                        'data': eh_stats
                    })
                break
        
        # 3. éªŒè¯æ•°æ®å®Œæ•´æ€§
        total_records = len(sfc_df)
        required_fields = sfc_config.get("required_fields", ['BatchNumber', 'Operation', 'TrackOutTime', 'machine'])
        non_null_checks = {}
        
        for field in required_fields:
            if field in sfc_df.columns:
                non_null_checks[field] = sfc_df[field].notna().sum()
        
        completeness = {k: f"{v}/{total_records} ({v/total_records*100:.1f}%)" for k, v in non_null_checks.items()}
        results['details'].append({
            'type': 'æ•°æ®å®Œæ•´æ€§',
            'data': completeness
        })
        
    except Exception as e:
        results['status'] = 'error'
        results['errors'].append({'type': 'éªŒè¯è¿‡ç¨‹é”™è¯¯', 'message': str(e)})
    
    return results


def validate_mes_calculations(mes_df: pd.DataFrame, sample_size: int = 10) -> Dict[str, Any]:
    """
    éªŒè¯MESæ•°æ®çš„å…³é”®è®¡ç®—ç»“æœ
    Args:
        mes_df: MESå¤„ç†åçš„æ•°æ®
        sample_size: æŠ½æ ·æ•°é‡
    Returns:
        éªŒè¯ç»“æœå­—å…¸
    """
    results = {'status': 'success', 'details': [], 'errors': []}
    
    try:
        # 1. éªŒè¯SFCæ•°æ®åˆå¹¶
        if 'Checkin_SFC' in mes_df.columns:
            merged_count = mes_df['Checkin_SFC'].notna().sum()
            total_count = len(mes_df)
            merge_rate = merged_count / total_count * 100
            
            results['details'].append({
                'type': 'SFCæ•°æ®åˆå¹¶ç»Ÿè®¡',
                'æ€»è®°å½•æ•°': total_count,
                'æˆåŠŸåˆå¹¶æ•°': merged_count,
                'åˆå¹¶ç‡': f"{merge_rate:.1f}%"
            })
        
        # 2. éªŒè¯æ ‡å‡†æ—¶é—´åŒ¹é…
        std_time_cols = ['EH_machine(s)', 'EH_labor(s)', 'OEE']
        matched_stats = {}
        
        for col in std_time_cols:
            if col in mes_df.columns:
                matched = mes_df[col].notna().sum()
                matched_stats[col] = f"{matched}/{len(mes_df)} ({matched/len(mes_df)*100:.1f}%)"
        
        if matched_stats:
            results['details'].append({
                'type': 'æ ‡å‡†æ—¶é—´åŒ¹é…ç»Ÿè®¡',
                'data': matched_stats
            })
        
        # 3. éªŒè¯DueTimeè®¡ç®—
        if all(col in mes_df.columns for col in ['TrackOutTime', 'DueTime']):
            valid_data = mes_df.dropna(subset=['TrackOutTime', 'DueTime'])
            if len(valid_data) > 0:
                sample_df = valid_data.sample(min(sample_size, len(valid_data)))
                
                for idx, row in sample_df.iterrows():
                    track_out = pd.to_datetime(row['TrackOutTime'])
                    due_time = pd.to_datetime(row['DueTime'])
                    
                    # DueTimeåº”è¯¥å¤§äºTrackOutTime
                    if due_time <= track_out:
                        results['errors'].append({
                            'type': 'DueTimeè®¡ç®—é”™è¯¯',
                            'index': idx,
                            'BatchNumber': row.get('BatchNumber', ''),
                            'TrackOutTime': str(track_out),
                            'DueTime': str(due_time)
                        })
        
        # 4. éªŒè¯PreviousBatchEndTimeè®¡ç®—
        if 'PreviousBatchEndTime' in mes_df.columns:
            # æ£€æŸ¥æ—¶é—´åºåˆ—çš„åˆç†æ€§
            valid_time_data = mes_df.dropna(subset=['TrackOutTime', 'PreviousBatchEndTime'])
            if len(valid_time_data) > 0:
                # æŒ‰machineå’ŒTrackOutTimeæ’åº
                sorted_data = valid_time_data.sort_values(['machine', 'TrackOutTime'])
                
                # æŠ½æ ·æ£€æŸ¥è¿ç»­æ‰¹æ¬¡çš„æ—¶é—´å…³ç³»
                sample_machines = sorted_data['machine'].unique()[:min(5, len(sorted_data['machine'].unique()))]
                
                for machine in sample_machines:
                    machine_data = sorted_data[sorted_data['machine'] == machine]
                    if len(machine_data) > 1:
                        # æ£€æŸ¥å‰å‡ ä¸ªæ‰¹æ¬¡çš„æ—¶é—´å…³ç³»
                        for i in range(1, min(4, len(machine_data))):
                            prev_end = pd.to_datetime(machine_data.iloc[i-1]['PreviousBatchEndTime'])
                            curr_start = pd.to_datetime(machine_data.iloc[i]['TrackOutTime'])
                            
                            # PreviousBatchEndTimeåº”è¯¥å°äºå½“å‰æ‰¹æ¬¡çš„TrackOutTime
                            if prev_end >= curr_start:
                                results['errors'].append({
                                    'type': 'PreviousBatchEndTimeè®¡ç®—é”™è¯¯',
                                    'machine': machine,
                                    'å½“å‰æ‰¹æ¬¡': str(machine_data.iloc[i]['BatchNumber']),
                                    'PreviousBatchEndTime': str(prev_end),
                                    'TrackOutTime': str(curr_start)
                                })
        
    except Exception as e:
        results['status'] = 'error'
        results['errors'].append({'type': 'éªŒè¯è¿‡ç¨‹é”™è¯¯', 'message': str(e)})
    
    return results


def validate_sap_routing_data(routing_df: pd.DataFrame) -> Dict[str, Any]:
    """
    éªŒè¯SAP Routingæ•°æ®çš„å®Œæ•´æ€§
    Args:
        routing_df: SAP Routingå¤„ç†åçš„æ•°æ®
    Returns:
        éªŒè¯ç»“æœå­—å…¸
    """
    results = {'status': 'success', 'details': [], 'errors': []}
    
    try:
        # 1. éªŒè¯å…³é”®å­—æ®µå®Œæ•´æ€§
        required_fields = ['Material Number', 'Operation', 'Group']
        completeness = {}
        
        for field in required_fields:
            if field in routing_df.columns:
                valid_count = routing_df[field].notna().sum()
                completeness[field] = f"{valid_count}/{len(routing_df)} ({valid_count/len(routing_df)*100:.1f}%)"
            else:
                completeness[field] = "å­—æ®µä¸å­˜åœ¨"
                results['errors'].append({'type': 'ç¼ºå°‘å…³é”®å­—æ®µ', 'field': field})
        
        results['details'].append({
            'type': 'SAP Routingæ•°æ®å®Œæ•´æ€§',
            'data': completeness
        })
        
        # 2. éªŒè¯æ ‡å‡†æ—¶é—´æ•°æ®åˆç†æ€§
        time_fields = ['EH_machine(s)', 'EH_labor(s)', 'OEE']
        time_stats = {}
        
        for field in time_fields:
            if field in routing_df.columns:
                valid_data = routing_df[routing_df[field].notna() & (routing_df[field] > 0)]
                if len(valid_data) > 0:
                    time_stats[field] = {
                        'æœ‰æ•ˆè®°å½•æ•°': len(valid_data),
                        'æœ€å°å€¼': round(valid_data[field].min(), 2),
                        'æœ€å¤§å€¼': round(valid_data[field].max(), 2),
                        'å¹³å‡å€¼': round(valid_data[field].mean(), 2)
                    }
        
        if time_stats:
            results['details'].append({
                'type': 'æ ‡å‡†æ—¶é—´æ•°æ®ç»Ÿè®¡',
                'data': time_stats
            })
        
        # 3. éªŒè¯æ•°æ®å”¯ä¸€æ€§
        if 'Material Number' in routing_df.columns and 'Operation' in routing_df.columns:
            duplicate_check = routing_df.duplicated(subset=['Material Number', 'Operation'])
            duplicate_count = duplicate_check.sum()
            
            results['details'].append({
                'type': 'æ•°æ®å”¯ä¸€æ€§æ£€æŸ¥',
                'æ€»è®°å½•æ•°': len(routing_df),
                'é‡å¤è®°å½•æ•°': duplicate_count,
                'å”¯ä¸€è®°å½•æ•°': len(routing_df) - duplicate_count
            })
        
    except Exception as e:
        results['status'] = 'error'
        results['errors'].append({'type': 'éªŒè¯è¿‡ç¨‹é”™è¯¯', 'message': str(e)})
    
    return results


def generate_validation_report() -> str:
    """
    ç”Ÿæˆå®Œæ•´çš„éªŒè¯æŠ¥å‘Š
    Returns:
        æŠ¥å‘Šæ–‡æœ¬
    """
    report_lines = []
    report_lines.append("=" * 80)
    report_lines.append("SAæŒ‡æ ‡ETLæ•°æ®æ¸…æ´—ç»“æœéªŒè¯æŠ¥å‘Š")
    report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("=" * 80)
    report_lines.append("")
    
    try:
        # éªŒè¯SFCæ•°æ®
        report_lines.append("1. SFCæ‰¹æ¬¡æŠ¥å·¥æ•°æ®éªŒè¯")
        report_lines.append("-" * 40)
        sfc_df = load_latest_data('sfc')
        sfc_results = validate_sfc_calculations(sfc_df)
        
        if sfc_results['status'] == 'success' and len(sfc_results['errors']) == 0:
            report_lines.append(f"âœ… SFCæ•°æ®éªŒè¯é€šè¿‡")
        else:
            report_lines.append(f"âŒ SFCæ•°æ®éªŒè¯å‘ç°é—®é¢˜")
        
        # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡
        for detail in sfc_results['details']:
            if detail['type'] == 'æ•°æ®å®Œæ•´æ€§':
                report_lines.append(f"   ğŸ“Š {detail['type']}:")
                for field, completeness in detail['data'].items():
                    report_lines.append(f"      - {field}: {completeness}")
            elif detail['type'] == 'æ ‡å‡†æ—¶é—´ç»Ÿè®¡':
                report_lines.append(f"   ğŸ“Š {detail['type']}:")
                for key, value in detail['data'].items():
                    report_lines.append(f"      - {key}: {value}")
        
        if sfc_results['errors']:
            report_lines.append(f"   âš ï¸ å‘ç° {len(sfc_results['errors'])} ä¸ªé”™è¯¯:")
            for error in sfc_results['errors'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
                if error['type'] == 'æ—¶é—´è®¡ç®—å·®å¼‚':
                    report_lines.append(f"      * {error['type']}: æ‰¹æ¬¡{error['BatchNumber']}, å·¥åº{error['Operation']}")
                    report_lines.append(f"        LTå·®å¼‚: {error['LTå·®å¼‚']}h, PTå·®å¼‚: {error['PTå·®å¼‚']}h, STå·®å¼‚: {error['STå·®å¼‚']}h")
                else:
                    report_lines.append(f"      * {error['type']}: {error.get('message', '')}")
        
        report_lines.append("")
        
        # éªŒè¯MESæ•°æ®
        report_lines.append("2. MESæ‰¹æ¬¡æŠ¥å·¥æ•°æ®éªŒè¯")
        report_lines.append("-" * 40)
        mes_df = load_latest_data('mes')
        mes_results = validate_mes_calculations(mes_df)
        
        if mes_results['status'] == 'success' and len(mes_results['errors']) == 0:
            report_lines.append(f"âœ… MESæ•°æ®éªŒè¯é€šè¿‡")
        else:
            report_lines.append(f"âŒ MESæ•°æ®éªŒè¯å‘ç°é—®é¢˜")
        
        # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡
        for detail in mes_results['details']:
            if detail['type'] in ['SFCæ•°æ®åˆå¹¶ç»Ÿè®¡', 'æ ‡å‡†æ—¶é—´åŒ¹é…ç»Ÿè®¡']:
                report_lines.append(f"   ğŸ“Š {detail['type']}:")
                for key, value in detail.items():
                    if key != 'type':
                        report_lines.append(f"      - {key}: {value}")
        
        if mes_results['errors']:
            report_lines.append(f"   âš ï¸ å‘ç° {len(mes_results['errors'])} ä¸ªé”™è¯¯:")
            for error in mes_results['errors'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
                report_lines.append(f"      * {error['type']}: {error.get('BatchNumber', error.get('machine', ''))}")
        
        report_lines.append("")
        
        # éªŒè¯SAP Routingæ•°æ®
        report_lines.append("3. SAP Routingæ ‡å‡†æ—¶é—´æ•°æ®éªŒè¯")
        report_lines.append("-" * 40)
        routing_df = load_latest_data('sap_routing')
        routing_results = validate_sap_routing_data(routing_df)
        
        if routing_results['status'] == 'success' and len(routing_results['errors']) == 0:
            report_lines.append(f"âœ… SAP Routingæ•°æ®éªŒè¯é€šè¿‡")
        else:
            report_lines.append(f"âŒ SAP Routingæ•°æ®éªŒè¯å‘ç°é—®é¢˜")
        
        # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡
        for detail in routing_results['details']:
            if detail['type'] in ['SAP Routingæ•°æ®å®Œæ•´æ€§', 'æ ‡å‡†æ—¶é—´æ•°æ®ç»Ÿè®¡', 'æ•°æ®å”¯ä¸€æ€§æ£€æŸ¥']:
                report_lines.append(f"   ğŸ“Š {detail['type']}:")
                for key, value in detail.items():
                    if key != 'type':
                        if isinstance(value, dict):
                            for sub_key, sub_value in value.items():
                                report_lines.append(f"      - {sub_key}: {sub_value}")
                        else:
                            report_lines.append(f"      - {key}: {value}")
        
        if routing_results['errors']:
            report_lines.append(f"   âš ï¸ å‘ç° {len(routing_results['errors'])} ä¸ªé”™è¯¯:")
            for error in routing_results['errors'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
                report_lines.append(f"      * {error['type']}: {error.get('message', '')}")
        
        report_lines.append("")
        
        # æ€»ç»“
        total_errors = len(sfc_results['errors']) + len(mes_results['errors']) + len(routing_results['errors'])
        report_lines.append("=" * 80)
        report_lines.append("éªŒè¯æ€»ç»“")
        report_lines.append("=" * 80)
        
        if total_errors == 0:
            report_lines.append("ğŸ‰ æ‰€æœ‰æ•°æ®éªŒè¯é€šè¿‡ï¼æ¸…æ´—ç»“æœæ­£ç¡®ã€‚")
        else:
            report_lines.append(f"âš ï¸ å‘ç° {total_errors} ä¸ªé—®é¢˜ï¼Œå»ºè®®è¯¦ç»†æ£€æŸ¥ã€‚")
        
        report_lines.append("")
        report_lines.append(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        report_lines.append(f"   - SFCæ•°æ®è®°å½•æ•°: {len(sfc_df):,}")
        report_lines.append(f"   - MESæ•°æ®è®°å½•æ•°: {len(mes_df):,}")
        report_lines.append(f"   - SAP Routingæ•°æ®è®°å½•æ•°: {len(routing_df):,}")
        
        # æ•°æ®è´¨é‡è¯„åˆ†
        quality_score = max(0, 100 - total_errors * 2)  # æ¯ä¸ªé”™è¯¯æ‰£2åˆ†
        report_lines.append(f"   - æ•°æ®è´¨é‡è¯„åˆ†: {quality_score}/100")
        
    except Exception as e:
        report_lines.append(f"âŒ éªŒè¯è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}")
    
    return "\n".join(report_lines)


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    log_config = cfg.get("logging", {
        'level': 'INFO',
        'file': 'logs/etl_validate_sa.log'
    })
    setup_logging(log_config)
    
    logging.info("å¼€å§‹SAæŒ‡æ ‡æ•°æ®æ¸…æ´—ç»“æœéªŒè¯")
    
    try:
        # ç”ŸæˆéªŒè¯æŠ¥å‘Š
        report = generate_validation_report()
        
        # è¾“å‡ºæŠ¥å‘Šåˆ°æ§åˆ¶å°
        print(report)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_config = cfg.get("report", {})
        output_dir = report_config.get("output_dir", "logs")
        filename_format = report_config.get("filename_format", "validation_report_sa_{timestamp}.txt")
        timestamp_format = report_config.get("timestamp_format", "%Y%m%d_%H%M%S")
        
        timestamp = datetime.now().strftime(timestamp_format)
        report_file = os.path.join(output_dir, filename_format.format(timestamp=timestamp))
        
        os.makedirs(output_dir, exist_ok=True)
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logging.info(f"éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        print(f"\nğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
    except Exception as e:
        logging.error(f"éªŒè¯å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
